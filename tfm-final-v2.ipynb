{"metadata":{"colab":{"collapsed_sections":["yg95E1NLEHGz","thv9Ft_AEJqE","DuBEGG9EVTZ5","VM-4wOrFEOjo","5WFghuGDE6Yj","w0t19TDn8T6H","Wfr4u0GG_wop","pupw9LQcAKLX","D_a_cX3H-kol","qVovjgHcgBdb","FrnC754vDR0J","laah8taIAUV-","arHNZrAHysyS","fEePeu9f_9OA","msUNjZjz_9OA","nc7QYj39_9OB","CjqEQyet_9OB","xB23VMK0_9OC","UC9tzn69TY0E","dcI7_Eh-UWKH","43QjVwFwZcRm","JargLFiWYt1d","OQ3EuPZJZKES","5CbCRVys_o4c","xGpAp-Ua__m7","tsAA07MgAuMB","Qad2eVdSTpnT","Raq9K5zJcTcv","R8wmDEiFHtQF","QmXVs2ZKH4PC","YKl1x1LnJgMj","dT22ofjsJvPi","wO2gAFXgJ9kE","T10r9Wr_KFHb","4_nRKh7sKOCR"],"provenance":[]},"gpuClass":"standard","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"c79a59f57ec74827ad6e4778b8bf486d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7e8dd1a0999a49c082fbbc3fe7d7cee1","IPY_MODEL_3addb5f6e71a4449a80f44a9bd6c4cde","IPY_MODEL_d98263853a754e79b63f427289c966bb"],"layout":"IPY_MODEL_51d552a4dd7a4655a9fc47cbe2797cdc"}},"7e8dd1a0999a49c082fbbc3fe7d7cee1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cdcd3351670a4e4dbba4c4576d1b526b","placeholder":"​","style":"IPY_MODEL_88ac5007618347f691ca1393f94d9f3d","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"3addb5f6e71a4449a80f44a9bd6c4cde":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ffc818b7a77f4ec282869f7daab6b9cb","max":247723,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6f77949ccf5844b698abac193d37efbf","value":247723}},"d98263853a754e79b63f427289c966bb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e7141a9effd40419b3c0674fada151f","placeholder":"​","style":"IPY_MODEL_80ccba580ab7455588749b3f9ce8bd7b","value":" 248k/248k [00:00&lt;00:00, 730kB/s]"}},"51d552a4dd7a4655a9fc47cbe2797cdc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cdcd3351670a4e4dbba4c4576d1b526b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88ac5007618347f691ca1393f94d9f3d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ffc818b7a77f4ec282869f7daab6b9cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f77949ccf5844b698abac193d37efbf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4e7141a9effd40419b3c0674fada151f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80ccba580ab7455588749b3f9ce8bd7b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5e8f0ca1e05440aa99e4acd83f1af943":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8a66f54c8f31436ea2a9c44fffb9d227","IPY_MODEL_f9d3168a0d71410d8403b2b0c5ea9f9d","IPY_MODEL_ba126fd3a3454d7f86faa65f0b6e86c9"],"layout":"IPY_MODEL_ead8cdc079c646738ead02a721e51eb8"}},"8a66f54c8f31436ea2a9c44fffb9d227":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d491b0f7bf9f44e99374b49a6642a445","placeholder":"​","style":"IPY_MODEL_fc784a2cb8d34dadbcf4259e53e9e1ae","value":"Downloading (…)cial_tokens_map.json: 100%"}},"f9d3168a0d71410d8403b2b0c5ea9f9d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4924a5cc4b764c178b0b5ea23f3667eb","max":134,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7a50c38bd4c846c4b1e1331d7598a730","value":134}},"ba126fd3a3454d7f86faa65f0b6e86c9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ce7e041a27045c981d291cc3e55e404","placeholder":"​","style":"IPY_MODEL_c75a6f3f60c544668866d18ec16a79ec","value":" 134/134 [00:00&lt;00:00, 7.33kB/s]"}},"ead8cdc079c646738ead02a721e51eb8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d491b0f7bf9f44e99374b49a6642a445":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc784a2cb8d34dadbcf4259e53e9e1ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4924a5cc4b764c178b0b5ea23f3667eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a50c38bd4c846c4b1e1331d7598a730":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9ce7e041a27045c981d291cc3e55e404":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c75a6f3f60c544668866d18ec16a79ec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2410203b98c64b98aff95c6ea67efd0c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9a35472029de463c9aec7cad4522caca","IPY_MODEL_846894f1e09f4807819fa6eee5343d45","IPY_MODEL_d6d1981d2512449995cdd3a01dcb5ba4"],"layout":"IPY_MODEL_9da63116c8254ebdb946cff4a5799631"}},"9a35472029de463c9aec7cad4522caca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ddee723ca3364b5aa377da68c72e74fe","placeholder":"​","style":"IPY_MODEL_2a6f14b160ed465d8ce3f2eb0a81bc00","value":"Downloading (…)okenizer_config.json: 100%"}},"846894f1e09f4807819fa6eee5343d45":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0def1b6230604f8e94a3ea3e6056a5fa","max":310,"min":0,"orientation":"horizontal","style":"IPY_MODEL_96cb8b4c75ee46a59df7ba8bea0e92fb","value":310}},"d6d1981d2512449995cdd3a01dcb5ba4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_41948ac3572c45058d19a01ca05e7d09","placeholder":"​","style":"IPY_MODEL_c88e40e4524f43549b8d2bc2fdc4da6a","value":" 310/310 [00:00&lt;00:00, 14.8kB/s]"}},"9da63116c8254ebdb946cff4a5799631":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ddee723ca3364b5aa377da68c72e74fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a6f14b160ed465d8ce3f2eb0a81bc00":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0def1b6230604f8e94a3ea3e6056a5fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96cb8b4c75ee46a59df7ba8bea0e92fb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"41948ac3572c45058d19a01ca05e7d09":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c88e40e4524f43549b8d2bc2fdc4da6a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c9451f9f3bd4927b3f23d9dc84e901f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a4bd592346984693834dd87d47835964","IPY_MODEL_fc93d27b574547bf88412db399e4df7f","IPY_MODEL_4f98c70406034557ac3bf46334739a19"],"layout":"IPY_MODEL_a34209d0c913446881813f7858a0607f"}},"a4bd592346984693834dd87d47835964":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2abf10ccc56e4577a1836f7b741b278d","placeholder":"​","style":"IPY_MODEL_965bf54deee0420fa1d8d0a3e5b477b1","value":"Downloading (…)lve/main/config.json: 100%"}},"fc93d27b574547bf88412db399e4df7f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f513af1a1334a6497b2b54a16b0548e","max":650,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e13a6c2179174497bed2cd44a98f8da3","value":650}},"4f98c70406034557ac3bf46334739a19":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6bf7da0fb34a4c98a8e95468f4708d79","placeholder":"​","style":"IPY_MODEL_e19b157d739942819bd177be0183ec44","value":" 650/650 [00:00&lt;00:00, 30.0kB/s]"}},"a34209d0c913446881813f7858a0607f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2abf10ccc56e4577a1836f7b741b278d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"965bf54deee0420fa1d8d0a3e5b477b1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8f513af1a1334a6497b2b54a16b0548e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e13a6c2179174497bed2cd44a98f8da3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6bf7da0fb34a4c98a8e95468f4708d79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e19b157d739942819bd177be0183ec44":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f87ef69133204570a1f37b77a60ef1fa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d9429437ebc443aea678c77c687249b0","IPY_MODEL_e1c24fc426764fabaf77563e53a3a559","IPY_MODEL_0a75e07a74e644b09c614513f9fecabc"],"layout":"IPY_MODEL_7f5bd67947f94cef94a3558a1cde5920"}},"d9429437ebc443aea678c77c687249b0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ee3dabe34654d39ae093480c9073465","placeholder":"​","style":"IPY_MODEL_7dbdefad365843dbbdc834918c2a4223","value":""}},"e1c24fc426764fabaf77563e53a3a559":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_86b04786d5d64145997b9a2bc4755160","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7d229e3a54624f8396db310e4e84eede","value":1}},"0a75e07a74e644b09c614513f9fecabc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_42f7e96ecccd433098fc7586c90e0fd0","placeholder":"​","style":"IPY_MODEL_2531e11e03294f30a22107be4e4fc9ed","value":" 36489/? [00:40&lt;00:00, 1079.74it/s]"}},"7f5bd67947f94cef94a3558a1cde5920":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ee3dabe34654d39ae093480c9073465":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7dbdefad365843dbbdc834918c2a4223":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"86b04786d5d64145997b9a2bc4755160":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"7d229e3a54624f8396db310e4e84eede":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"42f7e96ecccd433098fc7586c90e0fd0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2531e11e03294f30a22107be4e4fc9ed":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a41d7a5d0aa841808d29426f1e90e197":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4abf629439d24e43bbadeb8bddd516b3","IPY_MODEL_531d8569f3e2448580026c6e9b9459bd","IPY_MODEL_e0f8c0cbc5f8411b8dbe8a2355c14cb8"],"layout":"IPY_MODEL_6d9bf00ab51f413c9618f1b2af9b5c5e"}},"4abf629439d24e43bbadeb8bddd516b3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_17e98681acc14d22ae9cd2e9d9dbd187","placeholder":"​","style":"IPY_MODEL_ff7afdd3c5fd471ea4cc4d696de011f0","value":"Downloading (…)&quot;tf_model.h5&quot;;: 100%"}},"531d8569f3e2448580026c6e9b9459bd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c446b9dae4834c0d919ee1a45196b2ae","max":536635336,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5512625204554d6f8ebeb1e6d696c65e","value":536635336}},"e0f8c0cbc5f8411b8dbe8a2355c14cb8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d664afada3444dcbd1d2b7f7a78b4cd","placeholder":"​","style":"IPY_MODEL_930f9acc52d348df8291679b9e4651bb","value":" 537M/537M [00:03&lt;00:00, 185MB/s]"}},"6d9bf00ab51f413c9618f1b2af9b5c5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17e98681acc14d22ae9cd2e9d9dbd187":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff7afdd3c5fd471ea4cc4d696de011f0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c446b9dae4834c0d919ee1a45196b2ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5512625204554d6f8ebeb1e6d696c65e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9d664afada3444dcbd1d2b7f7a78b4cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"930f9acc52d348df8291679b9e4651bb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"53a1f2a83f764b5ca802623bb42daa42":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7c038ffdefb84a97a6de0751cb0dbb85","IPY_MODEL_8932df5caa77418d9cf35610d98a898c","IPY_MODEL_e29718048e094feeb3d0b180df108ce4"],"layout":"IPY_MODEL_4ad5c164194348e48a5a5aca97fc2314"}},"7c038ffdefb84a97a6de0751cb0dbb85":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_163034a7e1f1409b94969d6e50c6495e","placeholder":"​","style":"IPY_MODEL_e040abd257d34e1bb8019c58fa554879","value":""}},"8932df5caa77418d9cf35610d98a898c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a3da567311294763a912d18b0b59ff72","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c0897fbc389543eb818e4ecd45c743b1","value":1}},"e29718048e094feeb3d0b180df108ce4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b54daffd36c14a6d94db71c20561e73b","placeholder":"​","style":"IPY_MODEL_1a9039a1771249998e1cc39c644f3de9","value":" 36489/? [00:34&lt;00:00, 1507.09it/s]"}},"4ad5c164194348e48a5a5aca97fc2314":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"163034a7e1f1409b94969d6e50c6495e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e040abd257d34e1bb8019c58fa554879":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a3da567311294763a912d18b0b59ff72":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"c0897fbc389543eb818e4ecd45c743b1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b54daffd36c14a6d94db71c20561e73b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a9039a1771249998e1cc39c644f3de9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"90de54d8c84b483abc4fda547dd74eee":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_394ac5f8d6c24c5eb5b9d4d408d7aa60","IPY_MODEL_0ee04c1d20c2455ebe1bd3ed2f9cb2c6","IPY_MODEL_c45691e35bca445f8bdfe439b78c313e"],"layout":"IPY_MODEL_c63a99685e3a46ab881fd28e5ddc7215"}},"394ac5f8d6c24c5eb5b9d4d408d7aa60":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f58514e29e1d4875ad8e4d86bd179be0","placeholder":"​","style":"IPY_MODEL_7556e16154c34b418a85ae8a2223a921","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"0ee04c1d20c2455ebe1bd3ed2f9cb2c6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f20cbf34541c4bbb94c3c0415838cff5","max":213450,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b0296bd79e234a33a6276305e38fc03c","value":213450}},"c45691e35bca445f8bdfe439b78c313e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b1cd050477046dfa4815e3903abf4bd","placeholder":"​","style":"IPY_MODEL_bbeb92617d4843738e3001ae6402833d","value":" 213k/213k [00:00&lt;00:00, 831kB/s]"}},"c63a99685e3a46ab881fd28e5ddc7215":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f58514e29e1d4875ad8e4d86bd179be0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7556e16154c34b418a85ae8a2223a921":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f20cbf34541c4bbb94c3c0415838cff5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0296bd79e234a33a6276305e38fc03c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0b1cd050477046dfa4815e3903abf4bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bbeb92617d4843738e3001ae6402833d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"690f7105ee2a49a7a95aa4ffcf9fc924":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_26bdd9977f9749b69c2c66b9642fdf76","IPY_MODEL_0453506d837c4f8db3d7f10366407608","IPY_MODEL_3ab3eaf5ebf746339841776cea9ce760"],"layout":"IPY_MODEL_87bd52749402444cb0f37be7894d6c53"}},"26bdd9977f9749b69c2c66b9642fdf76":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8956647344c443e981fac4f67c7dad97","placeholder":"​","style":"IPY_MODEL_4bcf5dc621d747cc8623148fb05e6dac","value":"Downloading (…)okenizer_config.json: 100%"}},"0453506d837c4f8db3d7f10366407608":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_78f03204d8194c0dbfeef04aa56e21dd","max":29,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5680145429024b298711c4b376702091","value":29}},"3ab3eaf5ebf746339841776cea9ce760":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f66e4273d0bd45d6999c92c68d274e9b","placeholder":"​","style":"IPY_MODEL_55cd0a51c6754d90b1626250d0c98dc3","value":" 29.0/29.0 [00:00&lt;00:00, 1.78kB/s]"}},"87bd52749402444cb0f37be7894d6c53":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8956647344c443e981fac4f67c7dad97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4bcf5dc621d747cc8623148fb05e6dac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"78f03204d8194c0dbfeef04aa56e21dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5680145429024b298711c4b376702091":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f66e4273d0bd45d6999c92c68d274e9b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55cd0a51c6754d90b1626250d0c98dc3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"917cebfbfa674cb39fe215f12fdf3cc1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e86c4c565e12461cbdbc0a206e350b7a","IPY_MODEL_ee607256762a44fa9eb13352ab6632bb","IPY_MODEL_586cecfbc4094fc0b2fe19572db86984"],"layout":"IPY_MODEL_b8afb40c835942ed8725430344524baf"}},"e86c4c565e12461cbdbc0a206e350b7a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_03b3d80b595841f1923834b9111934cd","placeholder":"​","style":"IPY_MODEL_54f2117438f64091be0d740e6448d5b5","value":"Downloading (…)lve/main/config.json: 100%"}},"ee607256762a44fa9eb13352ab6632bb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_46b10644ca03466da3caeb2d58188a41","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d3934bcb57fb48b4b1063926240da9d4","value":570}},"586cecfbc4094fc0b2fe19572db86984":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c1c5a0e07b0c46ed9923c1f094391c4f","placeholder":"​","style":"IPY_MODEL_d778019c308f4bb888c9e11296db1dd5","value":" 570/570 [00:00&lt;00:00, 27.5kB/s]"}},"b8afb40c835942ed8725430344524baf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03b3d80b595841f1923834b9111934cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54f2117438f64091be0d740e6448d5b5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"46b10644ca03466da3caeb2d58188a41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3934bcb57fb48b4b1063926240da9d4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c1c5a0e07b0c46ed9923c1f094391c4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d778019c308f4bb888c9e11296db1dd5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c0cdadd2914c42c2b74cd4cee87bca39":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_24cee8a9b4d94340b6069f4bd3fd95a6","IPY_MODEL_992e0de953684e249ac8ead9f5624025","IPY_MODEL_61f357fefed8478fb45348a5af31c6d9"],"layout":"IPY_MODEL_d7fc855100144198a732c3f2920a18ab"}},"24cee8a9b4d94340b6069f4bd3fd95a6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b34bbdacac004775a3c8a53af9d23f21","placeholder":"​","style":"IPY_MODEL_74c58560d40d44408c71b3429146434c","value":""}},"992e0de953684e249ac8ead9f5624025":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_296613651f8b4867a48ead41657e9f33","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_409d3de5ea0b4b9dabbb792e11d3579c","value":1}},"61f357fefed8478fb45348a5af31c6d9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6c669cc71504b79a3352bf7f892c114","placeholder":"​","style":"IPY_MODEL_4875ae98ac10423cb215586feaa60b0b","value":" 36489/? [00:48&lt;00:00, 1420.41it/s]"}},"d7fc855100144198a732c3f2920a18ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b34bbdacac004775a3c8a53af9d23f21":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74c58560d40d44408c71b3429146434c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"296613651f8b4867a48ead41657e9f33":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"409d3de5ea0b4b9dabbb792e11d3579c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d6c669cc71504b79a3352bf7f892c114":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4875ae98ac10423cb215586feaa60b0b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ad70ad6b60c14316ad58a320e31ac10e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_02305d605bde4e85bb13db9be23ed7dd","IPY_MODEL_100c5d1be6e24e8bb8462efa2431db10","IPY_MODEL_0ec87ad75470447d89287ac45b572ff5"],"layout":"IPY_MODEL_03303dcfe6984936b49e16f7cda89e94"}},"02305d605bde4e85bb13db9be23ed7dd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_df50db067e864c99b1c1091c670e56d5","placeholder":"​","style":"IPY_MODEL_93f7ff58d5ea489abe21e97a75a9e9b1","value":"Downloading (…)&quot;tf_model.h5&quot;;: 100%"}},"100c5d1be6e24e8bb8462efa2431db10":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_26c46ba02357475092a8576c8901738f","max":526681800,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3f9671cc6d174eebad0bac9703a1e50f","value":526681800}},"0ec87ad75470447d89287ac45b572ff5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_77491e77b99e4214b941682fec554729","placeholder":"​","style":"IPY_MODEL_e14a1c787e694f80a4825e0bc3438279","value":" 527M/527M [00:02&lt;00:00, 239MB/s]"}},"03303dcfe6984936b49e16f7cda89e94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df50db067e864c99b1c1091c670e56d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93f7ff58d5ea489abe21e97a75a9e9b1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"26c46ba02357475092a8576c8901738f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f9671cc6d174eebad0bac9703a1e50f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"77491e77b99e4214b941682fec554729":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e14a1c787e694f80a4825e0bc3438279":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d890551714f746399289baa0efaeeed7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1bcb84783315430fb3b11df19b0852b9","IPY_MODEL_f531b7dae0824d0f887e42a694a5cbdf","IPY_MODEL_3a8434e1d6924e69add602cee5fde465"],"layout":"IPY_MODEL_8452c24c77284b4f824012feaa7974a0"}},"1bcb84783315430fb3b11df19b0852b9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac5586cbcced41e5bc2e93c5d92ada82","placeholder":"​","style":"IPY_MODEL_e2c9bfd614b34ba2a18ad720a179f3f3","value":""}},"f531b7dae0824d0f887e42a694a5cbdf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe9754e08dd54c6eb70c562cce02cbae","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bbaf9f2e6b1a492da29d1bf8c8236f89","value":1}},"3a8434e1d6924e69add602cee5fde465":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2913a51ddb34f318d2bbb4a3daf5cdf","placeholder":"​","style":"IPY_MODEL_dc8c2e31f4c54cd08a28faee22da8742","value":" 36489/? [00:21&lt;00:00, 1940.48it/s]"}},"8452c24c77284b4f824012feaa7974a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac5586cbcced41e5bc2e93c5d92ada82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2c9bfd614b34ba2a18ad720a179f3f3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fe9754e08dd54c6eb70c562cce02cbae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"bbaf9f2e6b1a492da29d1bf8c8236f89":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e2913a51ddb34f318d2bbb4a3daf5cdf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc8c2e31f4c54cd08a28faee22da8742":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#Importando bases","metadata":{"id":"4L1LTG-3EBWd"}},{"cell_type":"code","source":"# KAGGLE: https://www.kaggle.com/code/andrscampaa/detecting-bullying-tweets-pytorch-lstm-bert/edit\n# Tutorial: https://www.youtube.com/watch?v=wp9BudYGZyA&t=1862s\n# Actualizando matplotlib\n#!pip install matplotlib==3.4\n#import os\n#os.kill(os.getpid(), 9)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gc8hlaeclU4d","outputId":"b6d374bf-ddb1-457d-c369-a99ed16f5171"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Descargando librerías necesarias\n!pip install transformers\n!pip install -U deep-translator\n!python -m spacy download es_core_news_md\n!python -m spacy download en_core_web_md\n\nimport matplotlib\nimport re\nimport es_core_news_md\nimport en_core_web_md\nimport pandas as pd \nimport base64\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef create_onedrive_directdownload (onedrive_link):\n    data_bytes64 = base64.b64encode(bytes(onedrive_link, 'utf-8'))\n    data_bytes64_String = data_bytes64.decode('utf-8').replace('/','_').replace('+','-').rstrip(\"=\")\n    resultUrl = f\"https://api.onedrive.com/v1.0/shares/u!{data_bytes64_String}/root/content\"\n    return resultUrl\n\n# Input any OneDrive URL \nonedrive_url = \"https://1drv.ms/x/s!AhJYFCPR8DpKiLluuU5Ia5sGNc7Jew?e=hoETSw\"\n# Generate Direct Download URL from above Script\ndirect_download_url = create_onedrive_directdownload(onedrive_url)\nprint(direct_download_url)\n\npp=0\ntry:\n  dff = pd.read_excel(direct_download_url)\n  print(\"Finale...\")\n  dff=dff.loc[~pd.isnull(dff[\"tweet_text_translated_clean\"])]\n  dff=dff.loc[~pd.isnull(dff[\"tweet_text_clean\"])]\n  dff.loc[dff[\"cyberbullying_type\"]==\"not_cyberbullying\",\"cyberbullying_type2\"]=0\n  dff.loc[dff[\"cyberbullying_type\"]!=\"not_cyberbullying\",\"cyberbullying_type2\"]=1\n  dff.loc[dff[\"cyberbullying_type\"]==\"not_cyberbullying\",\"cyberbullying_typex\"]=\"not_cyberbullying\"\n  dff.loc[dff[\"cyberbullying_type\"]!=\"not_cyberbullying\",\"cyberbullying_typex\"]=\"cyberbullying\"\n  dff.loc[dff[\"cyberbullying_type\"]==\"not_cyberbullying\",\"cyberbullying_type3\"]=0\n  dff.loc[dff[\"cyberbullying_type\"]==\"age\",\"cyberbullying_type3\"]=1\n  dff.loc[dff[\"cyberbullying_type\"]==\"gender\",\"cyberbullying_type3\"]=2\n  dff.loc[dff[\"cyberbullying_type\"]==\"ethnicity\",\"cyberbullying_type3\"]=3\n  dff.loc[dff[\"cyberbullying_type\"]==\"religion\",\"cyberbullying_type3\"]=4\n  dff.loc[dff[\"cyberbullying_type\"]==\"other_cyberbullying\",\"cyberbullying_type3\"]=5\n  dff = dff[~dff[\"tweet_text_clean\"].duplicated()]\n  dff = dff[~dff[\"tweet_text_translated_clean\"].duplicated()]\n  pp = 1\nexcept:\n  pass\n\nif pp!=1:\n  # Input any OneDrive URL (original)\n  onedrive_url = \"https://1drv.ms/u/s!AhJYFCPR8DpKiK4c1lzD4HoS8zDIQQ?e=poWnUK\"\n  # Generate Direct Download URL from above Script\n  direct_download_url = create_onedrive_directdownload(onedrive_url)\n  print(direct_download_url)\n\n  # Load Dataset to the Dataframe (traducida)\n  df = pd.read_csv(direct_download_url)\n  df[\"prediccion_jonatan\"]=\"\"\n  print(df.columns)\n  print(len(df))\n\n  # Input any OneDrive URL \n  onedrive_url = \"https://1drv.ms/x/s!AhJYFCPR8DpKiLZZbc3wCPMCbbHQzw?e=nF6YEf\"\n  # Generate Direct Download URL from above Script\n  direct_download_url = create_onedrive_directdownload(onedrive_url)\n  print(direct_download_url)\n\n  try:\n    df2 = pd.read_excel(direct_download_url)\n    dff = pd.concat([df,df2[\"tweet_text_translated\"]],axis=1)\n    dff[\"Unnamed: 0\"]=dff.index\n  except:\n    pass\n\nprint(\"Matplotlib version:\",matplotlib.__version__)\nprint(pp)\ntry:\n  print(\"Missings en tweets traducidos:\",dff[\"tweet_text_translated_clean\"].isna().sum())\n  print(\"Missings en tweets:\",dff[\"tweet_text_clean\"].isna().sum())\n  print(\"Duplicados de tweets:\",dff[\"tweet_text_clean\"].duplicated().sum())\n  print(\"Duplicados de tweets: traducidos:\",dff[\"tweet_text_translated_clean\"].duplicated().sum())\n  print(\"Base final tiene:\",len(dff))\nexcept:\n  pass","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5059Qg8W8noy","outputId":"c83d4511-6c68-42aa-c4f8-5d223b6d6b47","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Seleccionando base","metadata":{"id":"yg95E1NLEHGz"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom deep_translator import GoogleTranslator\n#from google.colab import files\n\nif pp!=1:\n  if \"tweet_text_translated\" not in dff.columns.tolist():\n    print(\"no hay\")\n    df[\"tweet_text_translated\"]=\"\"\n    X = df[['tweet_text','tweet_text_translated','prediccion_jonatan']]\n    y=df['cyberbullying_type']\n    z=\"\"\n\n    # Traducción\n    if z==\"\":\n      for j in [47000,48000]:\n        p=1\n        for i in range(j-1000,j):#len(X)):\n          try:\n            print(p)\n            translated = GoogleTranslator(source='auto', target='es').translate(X[\"tweet_text\"][i])\n            X[\"tweet_text_translated\"][i]=translated\n            p+=1\n            if i+1==j:\n              jj=j-1000\n              data = X.iloc[jj:j,:]\n              data.to_excel(\"aqui\"+str(i)+\".xlsx\")\n              #files.download(\"aqui\"+str(i)+\".xlsx\")\n          except:\n            p+=1\n            pass\n\n    else:\n      pass\n\n  else:\n    dff=dff.loc[~pd.isnull(dff[\"tweet_text_translated\"])]\n    print(\"Missings traducidos:\",dff.isna().sum())\n    dff[\"prediccion_jonatan\"] = \"\"\n    dff[\"Unnamed: 0\"] = dff.index\n    X = dff[[\"Unnamed: 0\",'tweet_text','tweet_text_translated','prediccion_jonatan']]\n    dff[\"cyberbullying_type2\"] = dff[\"cyberbullying_type\"]\n    dff.loc[dff[\"cyberbullying_type\"]!=\"not_cyberbullying\",\"cyberbullying_type\"]=\"Bullying\"\n    dff.loc[dff[\"cyberbullying_type\"]==\"not_cyberbullying\",\"cyberbullying_type\"]=\"Not_bullying\"\n    dff.loc[dff[\"cyberbullying_type\"]==\"Bullying\",\"cyberbullying_type\"]=1\n    dff.loc[dff[\"cyberbullying_type\"]==\"Not_bullying\",\"cyberbullying_type\"]=0\n    y = dff[\"cyberbullying_type\"].astype(int)\n\n# Limpiando nulls\nprint(len(dff))\nprint(\"casi listo\")\nX = dff[[\"Unnamed: 0\",'tweet_text','tweet_text_translated','prediccion_jonatan','tweet_text_translated_clean','tweet_text_clean']]\nyyy = dff[[\"cyberbullying_type2\",\"cyberbullying_type3\"]].astype(int)\n\n# Luego de ver resultados, igual se debe verificar con train-test-split\nX_train, X_test, y_train, y_test = train_test_split(X, yyy, test_size=0.2,random_state=17)\nprint(\"Hay por procesar:\", len(X_test))\nprint(X_test.head())\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import roc_auc_score","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iU_1tuf777qJ","outputId":"b9e92a1b-0b8a-424e-d53d-dd79ddb5dd2b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Funciones","metadata":{"id":"5ikoec31h8Zj"}},{"cell_type":"code","source":"# Pre procesamiento para NUEVA DATA\ndef get_features(rev):\n    obj = []\n    doc = re.sub(r'\\W', ' ', rev)\n    doc = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', doc) \n    doc = re.sub(r'\\^[a-zA-Z]\\s+', ' ', doc) \n    doc = re.sub(r'\\s+', ' ', doc, flags=re.I)\n    doc = re.sub(r'^b\\s+', '', doc)\n    doc = doc.lower()\n    doc = re.sub('rt', '', doc)\n    doc = re.sub('él', '', doc)\n    doc = re.sub('co', '', doc)\n    doc = re.sub('_', '', doc) \n    doc = nlp (doc)\n    lemmas = [no_stop.lemma_ for no_stop in doc if not no_stop.is_stop]\n    doc = ' '.join(lemmas)\n    doc = re.sub('él', '', doc)\n    doc = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', doc)\n    doc = re.sub(r'\\^[a-zA-Z]\\s+', ' ', doc)\n    obj.append(doc)\n    obj = vectorizer.transform(obj)\n    obj = tfidfconverter.transform(obj)\n    return obj\n\ndef cooler_confusion_matrix(y, y_pred, title, labels):\n    fig, ax =plt.subplots(figsize=(7.5,7.5))\n    ax=sns.heatmap(confusion_matrix(y, y_pred), annot=True, cmap=\"Blues\", fmt='g', cbar=False, annot_kws={\"size\":30})\n    plt.title(title, fontsize=25)\n    ax.xaxis.set_ticklabels(labels, fontsize=16) \n    ax.yaxis.set_ticklabels(labels, fontsize=14.5)\n    ax.set_ylabel('Test', fontsize=25)\n    ax.set_xlabel('Predicción', fontsize=25)\n    plt.show()","metadata":{"id":"jZgcpGKfh9PQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bi clase","metadata":{"id":"zbVU20P3pTmT"}},{"cell_type":"markdown","source":"#Español","metadata":{"id":"thv9Ft_AEJqE"}},{"cell_type":"markdown","source":"### Estadística descriptiva","metadata":{"id":"DuBEGG9EVTZ5"}},{"cell_type":"code","source":"# Modelando\nf1=[]\nacc=[]\nprec=[]\nspec=[]\nsens=[]\nvpp=[]\nvpn=[]\nauc=[]\nmod=[]\ndat=[]\n\ndef autopct_format(values):\n        def my_format(pct):\n            total = sum(values)\n            val = int(round(pct*total/100.0))\n            return '{:.1f}%\\n({v:d})'.format(pct, v=val)\n        return my_format\n\n# Pie 1\nlabels = ['religión', 'edad', 'género', 'etnicidad', 'no cyberbullying', 'otro tipo de cyberbullying']\ns = dff.cyberbullying_type.value_counts()\nplt.pie(s,labels = labels, autopct=autopct_format(s))\nplt.show()\n\n# Pie 2\nlabels = ['cyberbullying', 'no cyberbullying']\ns = dff.cyberbullying_typex.value_counts()\nplt.pie(s,labels = labels, autopct=autopct_format(s))\nplt.show()\n\n# Longitud\ntext_len = []\nfor text in dff[\"tweet_text_translated\"]:\n    tweet_len = len(text.split())\n    text_len.append(tweet_len)\n\ndff['text_translated_orig_len'] = text_len\nplt.figure(figsize=(7,5))\nax = sns.countplot(x='text_translated_orig_len', data=dff, palette='magma')\nplt.title('Conteo de textos originales por número de palabras (español)', fontsize=20)\nplt.yticks([])\nplt.xticks([])\n#for i in ax.containers:\n#    ax.bar_label(i,)\nplt.ylabel('conteo')\nplt.xlabel('')\nplt.show()\nprint(dff['text_translated_orig_len'].describe())\n\nfrom collections import Counter\nresults = set()\ndff['tweet_text_translated'].str.lower().str.split().apply(results.update)\nresults = Counter()\ndff['tweet_text_translated'].str.lower().str.split().apply(results.update)\nprint(len(results))\nprint(results.most_common())","metadata":{"id":"6C1QZGHVVTt5","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"cf310837-7476-48f9-ef55-e52659de966b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hugging face","metadata":{"id":"VM-4wOrFEOjo"}},{"cell_type":"code","source":"from transformers import pipeline\nmodel_path = \"JonatanGk/roberta-base-bne-finetuned-cyberbullying-spanish\"\nbullying_analysis = pipeline(\"text-classification\", model=model_path, tokenizer=model_path)\n\np=1\nfor i in X_test.index:\n  try:\n    print(p)\n    X_test[\"prediccion_jonatan\"][i]=bullying_analysis(X_test[\"tweet_text_translated\"][i])[0][\"label\"]\n    p+=1\n  except:\n    p+=1\n    pass\n\nfrom google.colab import files\ndata=X_test.join(y_test)\ndata.loc[data[\"prediccion_jonatan\"]==\"Bullying\",\"prediccion_jonatan\"]=1\ndata.loc[data[\"prediccion_jonatan\"]==\"Not_bullying\",\"prediccion_jonatan\"]=0\ndata.loc[data[\"prediccion_jonatan\"]==\"\",\"prediccion_jonatan\"]=0\ndata[\"prediccion_jonatan\"]=data[\"prediccion_jonatan\"].fillna(0)\n\nprint(\"Resultados Hugging face...\")\nprint(cooler_confusion_matrix(data[\"cyberbullying_type2\"].astype(int),data[\"prediccion_jonatan\"].astype(int),\"Aqui\",[\"Si\",\"No\"]))\nprint(classification_report(data[\"cyberbullying_type2\"].astype(int),data[\"prediccion_jonatan\"].astype(int)))\n\nmod.append(\"Hugging Face\")\nf1.append(round(f1_score(data[\"cyberbullying_type2\"].astype(int),data[\"prediccion_jonatan\"].astype(int), average='weighted'),2))\nacc.append(round(accuracy_score(data[\"cyberbullying_type2\"].astype(int),data[\"prediccion_jonatan\"].astype(int)),2))\nauc.append(round(roc_auc_score(data[\"cyberbullying_type2\"].astype(int),data[\"prediccion_jonatan\"].astype(int)),2))\nprec.append(round(precision_score(data[\"cyberbullying_type2\"].astype(int),data[\"prediccion_jonatan\"].astype(int)),2))\ntn, fp, fn, tp = confusion_matrix(data[\"cyberbullying_type2\"].astype(int),data[\"prediccion_jonatan\"].astype(int)).ravel()\nspecificity = tn / (tn+fp)\nsensitivity = tp / (tp+fn)\nspec.append(round(specificity,2))\nsens.append(round(sensitivity,2))\nvpps = tp / (tp+fp)\nvpns = tn / (tn+fn)\nvpp.append(round(vpps,2))\nvpn.append(round(vpns,2))\n\np=1\nfor i in X_test.index:\n  try:\n    print(p)\n    X_test[\"prediccion_jonatan\"][i]=bullying_analysis(X_test[\"tweet_text_translated_clean\"][i])[0][\"label\"]\n    p+=1\n  except:\n    p+=1\n    pass\n\nfrom google.colab import files\ndata=X_test.join(y_test)\ndata.loc[data[\"prediccion_jonatan\"]==\"Bullying\",\"prediccion_jonatan\"]=1\ndata.loc[data[\"prediccion_jonatan\"]==\"Not_bullying\",\"prediccion_jonatan\"]=0\ndata.loc[data[\"prediccion_jonatan\"]==\"\",\"prediccion_jonatan\"]=0\ndata[\"prediccion_jonatan\"]=data[\"prediccion_jonatan\"].fillna(0)\n\nprint(\"Resultados Hugging face...\")\nprint(cooler_confusion_matrix(data[\"cyberbullying_type2\"].astype(int),data[\"prediccion_jonatan\"].astype(int),\"Aqui\",[\"Si\",\"No\"]))\nprint(classification_report(data[\"cyberbullying_type2\"].astype(int),data[\"prediccion_jonatan\"].astype(int)))\n\nmod.append(\"Hugging Face clean\")\nf1.append(round(f1_score(data[\"cyberbullying_type2\"].astype(int),data[\"prediccion_jonatan\"].astype(int), average='weighted'),2))\nacc.append(round(accuracy_score(data[\"cyberbullying_type2\"].astype(int),data[\"prediccion_jonatan\"].astype(int)),2))\nauc.append(round(roc_auc_score(data[\"cyberbullying_type2\"].astype(int),data[\"prediccion_jonatan\"].astype(int)),2))\nprec.append(round(precision_score(data[\"cyberbullying_type2\"].astype(int),data[\"prediccion_jonatan\"].astype(int)),2))\ntn, fp, fn, tp = confusion_matrix(data[\"cyberbullying_type2\"].astype(int),data[\"prediccion_jonatan\"].astype(int)).ravel()\nspecificity = tn / (tn+fp)\nsensitivity = tp / (tp+fn)\nspec.append(round(specificity,2))\nsens.append(round(sensitivity,2))\nvpps = tp / (tp+fp)\nvpns = tn / (tn+fn)\nvpp.append(round(vpps,2))\nvpn.append(round(vpns,2))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"wy5viHKJEKAd","outputId":"6564e353-92b2-4af3-b686-a8987e76f22b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TF-IDF","metadata":{"id":"5WFghuGDE6Yj"}},{"cell_type":"code","source":"nlp = es_core_news_md.load()\ndocuments = []\nj = 0\nX = dff[[\"Unnamed: 0\",'tweet_text','tweet_text_translated','prediccion_jonatan']]\nXX = X[\"tweet_text_translated\"]\ny = yyy[\"cyberbullying_type2\"]\n\nif pp!=1:\n  for sen in XX.index:\n      # Removiendo caracteres especiales\n      document = re.sub(r'\\W', ' ', str(XX[sen]))\n      \n      # Removiendo caracteres solitarios despues del inicio\n      document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n      \n      # Removiendo caracteres solitarios al inicio\n      document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n\n      # Removiendo el prefijo b\n      document = re.sub(r'^b\\s+', '', document) #solo cuando se agrega al inicio\n      \n      # Minúsculas\n      document = document.lower()\n\n      # Removiendo caracteres solitarios al inicio\n      document = re.sub('rt', '', document)\n\n      # Removiendo caracteres solitarios al inicio\n      document = re.sub('http', '', document)\n\n      # Removiendo caracteres solitarios al inicio\n      document = re.sub('co', '', document)\n\n      # Removiendo caracteres solitarios al inicio\n      document = re.sub('_', '', document) \n\n      # Removiendo multiples espacios\n      document = re.sub(r'\\s+', ' ', document, flags=re.I)\n      \n      # Lematizacion\n      document = nlp (document)\n      lemmas = [no_stop.lemma_ for no_stop in document if not no_stop.is_stop]\n      document = ' '.join(lemmas)\n\n      # Removiendo caracteres solitarios al inicio\n      document = re.sub('él', '', document)\n\n      # Removiendo caracteres solitarios despues del inicio\n      document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n      \n      # Removiendo caracteres solitarios al inicio\n      document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n  \n      documents.append(document)\n      j = j+1\n      print(j)\n\n  # Longitud\n  text_len = []\n  for text in documents:\n      tweet_len = len(text.split())\n      text_len.append(tweet_len)\n\n  dff[\"tweet_text_translated_clean\"]=documents\n  dff['text_translated_clean_len'] = text_len\n  dff=dff.loc[~pd.isnull(dff[\"tweet_text_translated_clean\"])]\n\nplt.figure(figsize=(7,5))\nax = sns.countplot(x='text_translated_clean_len', data=dff, palette='magma')\nplt.title('Conteo de textos limpiados por número de palabras (español)', fontsize=20)\nplt.yticks([])\nplt.xticks([])\n#for i in ax.containers:\n#    ax.bar_label(i,)\nplt.ylabel('conteo')\nplt.xlabel('')\nplt.show()\nprint(dff['text_translated_clean_len'].describe())\n\nfrom collections import Counter\nresults = set()\ndff['tweet_text_translated_clean'].str.lower().str.split().apply(results.update)\nresults = Counter()\ndff['tweet_text_translated_clean'].str.lower().str.split().apply(results.update)\nprint(len(results))\nprint(results.most_common())\n\nyy = [count for tag, count in results.most_common(20)]\nx = [tag for tag, count in results.most_common(20)]\n\nplt.barh(x,yy, color='crimson')\n\n# Vectorizar\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(max_features=1000, min_df=5, max_df=0.7)\nXX = vectorizer.fit_transform(dff['tweet_text_translated_clean']).toarray()\n\n# TF-IDF\nfrom sklearn.feature_extraction.text import TfidfTransformer\ntfidfconverter = TfidfTransformer()\nXX = tfidfconverter.fit_transform(XX).toarray()\n\n# Separando (equivalente al de arriba por el random state)\nX_train, X_test, y_train, y_test = train_test_split(XX, y, test_size=0.2,random_state=17)\nprint(\"Hay por procesar:\", len(X_test))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":804},"id":"5Js31heNE6se","outputId":"338293d3-0256-4ec6-c69a-a83ea93103bf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  + RF","metadata":{"id":"w0t19TDn8T6H"}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=1000, random_state=0)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\n\n# Escribe tu propia reseña y prueba tu modelo\nreview1 = \"Perra loca\"\nreview1_features = get_features(review1)\nprint(\"Review:\", review1)\nprint(\"Probabilidad de bullying:\", rf.predict_proba(review1_features)[0,1]) #0 es por la primera obs, y 1 es por prob de posi\n\nmod.append(\"TF-IDF + RF\")\nf1.append(round(f1_score(y_test,y_pred, average='weighted'),2))\nacc.append(round(accuracy_score(y_test,y_pred),2))\nauc.append(round(roc_auc_score(y_test,y_pred),2))\nprec.append(round(precision_score(y_test,y_pred),2))\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\nspecificity = tn / (tn+fp)\nsensitivity = tp / (tp+fn)\nspec.append(round(specificity,2))\nsens.append(round(sensitivity,2))\nvpps = tp / (tp+fp)\nvpns = tn / (tn+fn)\nvpp.append(round(vpps,2))\nvpn.append(round(vpns,2))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PRbhdEh7374E","outputId":"ad0b5962-1adc-435c-f3a1-9e8c1123e0dd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### + SVM","metadata":{"id":"Wfr4u0GG_wop"}},{"cell_type":"code","source":"from sklearn.svm import SVC\nsv = SVC(random_state=17)\nsv.fit(X_train, y_train)\ny_pred = sv.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\n\n# Escribe tu propia reseña y prueba tu modelo\nreview1 = \"Perra loca\"\nreview1_features = get_features(review1)\nprint(\"Review:\", review1)\nprint(\"Probabilidad de bullying:\", sv.predict(review1_features.toarray())) #0 es por la primera obs, y 1 es por prob de posi\n\nmod.append(\"TF-IDF + SVM\")\nf1.append(round(f1_score(y_test,y_pred, average='weighted'),2))\nacc.append(round(accuracy_score(y_test,y_pred),2))\nauc.append(round(roc_auc_score(y_test,y_pred),2))\nprec.append(round(precision_score(y_test,y_pred),2))\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\nspecificity = tn / (tn+fp)\nsensitivity = tp / (tp+fn)\nspec.append(round(specificity,2))\nsens.append(round(sensitivity,2))\nvpps = tp / (tp+fp)\nvpns = tn / (tn+fn)\nvpp.append(round(vpps,2))\nvpn.append(round(vpns,2))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PmTzkUE9-H1-","outputId":"56354e63-296e-4bfa-c745-10abbfa21186"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### + Naive Bayes","metadata":{"id":"pupw9LQcAKLX"}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(X_train, y_train)\ny_pred = nb.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\n\n# Escribe tu propia reseña y prueba tu modelo\nreview1 = \"Puta\"\nreview1_features = get_features(review1)\nprint(\"Review:\", review1)\nprint(\"Probabilidad de bullying:\", nb.predict_proba(review1_features.toarray())[0,1]) #0 es por la primera obs, y 1 es por prob de posi\n\nmod.append(\"TF-IDF + Naive Bayes\")\nf1.append(round(f1_score(y_test,y_pred, average='weighted'),2))\nacc.append(round(accuracy_score(y_test,y_pred),2))\nauc.append(round(roc_auc_score(y_test,y_pred),2))\nprec.append(round(precision_score(y_test,y_pred),2))\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\nspecificity = tn / (tn+fp)\nsensitivity = tp / (tp+fn)\nspec.append(round(specificity,2))\nsens.append(round(sensitivity,2))\nvpps = tp / (tp+fp)\nvpns = tn / (tn+fn)\nvpp.append(round(vpps,2))\nvpn.append(round(vpns,2))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EPpficIxANNc","outputId":"833fc3a3-3252-41eb-9e1a-47d1300097e1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### + Redes neuronales","metadata":{"id":"D_a_cX3H-kol"}},{"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nnn = MLPClassifier(hidden_layer_sizes=(150,100,50,10), max_iter=300, activation = 'relu',solver='sgd',random_state=17)\nnn.fit(X_train, y_train)\ny_pred = nn.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\n\n# Escribe tu propia reseña y prueba tu modelo\nreview1 = \"Puta\"\nreview1_features = get_features(review1)\nprint(\"Review:\", review1)\nprint(\"Probabilidad de bullying:\", nn.predict_proba(review1_features.toarray())[0,1]) #0 es por la primera obs, y 1 es por prob de posi\n\nmod.append(\"TF-IDF + Redes Neuronales\")\nf1.append(round(f1_score(y_test,y_pred, average='weighted'),2))\nacc.append(round(accuracy_score(y_test,y_pred),2))\nauc.append(round(roc_auc_score(y_test,y_pred),2))\nprec.append(round(precision_score(y_test,y_pred),2))\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\nspecificity = tn / (tn+fp)\nsensitivity = tp / (tp+fn)\nspec.append(round(specificity,2))\nsens.append(round(sensitivity,2))\nvpps = tp / (tp+fp)\nvpns = tn / (tn+fn)\nvpp.append(round(vpps,2))\nvpn.append(round(vpns,2))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KvhkNKcj-lIG","outputId":"0f283c43-5f85-4487-e6ae-addcb42ac948"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### BETO","metadata":{"id":"qVovjgHcgBdb"}},{"cell_type":"code","source":"### Preparación general\n# Librerías para BETO\nfrom tqdm.auto import tqdm\nimport tensorflow as tf\nfrom transformers import BertTokenizer\nmodelo='dccuchile/bert-base-spanish-wwm-uncased'\nvariable_x=\"tweet_text_translated\"\nnombre_modelo='bullying_beto_modelo'\nz=16 # Batch size\nepocas=3 # N° épocas\n\nX = dff[[\"Unnamed: 0\",'tweet_text','tweet_text_translated','prediccion_jonatan']]\n# train-test-split para documents (textos sin íconos especiales ni doble espaciado)\nX_train, X_test, y_train, y_test = train_test_split(X[variable_x], y, test_size=0.2,random_state=17)\nprint(\"Hay por procesar:\", len(X_test))\nprint(X_test)\n\n# Trayendo al modelo de Hugging Face\ntokenizer = BertTokenizer.from_pretrained(modelo)\n\n# Tokenizando\ntoken = tokenizer.encode_plus(\n    X_train.iloc[0],\n    max_length=256, \n    truncation=True, \n    padding='max_length', \n    add_special_tokens=True,\n    return_tensors='tf'\n)\nprint(token.input_ids)\nX_input_ids = np.zeros((len(X_train), 256))\nX_attn_masks = np.zeros((len(X_train), 256))\n\n# Funciones\ndef generate_training_data(df, ids, masks, tokenizer):\n    for i, text in tqdm(enumerate(X_train)):\n        tokenized_text = tokenizer.encode_plus(\n            text,\n            max_length=256, \n            truncation=True, \n            padding='max_length', \n            add_special_tokens=True,\n            return_tensors='tf'\n        )\n        ids[i, :] = tokenized_text.input_ids\n        masks[i, :] = tokenized_text.attention_mask\n    return ids, masks\n\n# Creando efectivamente la tokenización\nX_input_ids, X_attn_masks = generate_training_data(X_train, X_input_ids, X_attn_masks, tokenizer)\nlabels = np.zeros((len(X_train), 2))\nlabels[np.arange(len(X_train)), y_train] = 1\n\n# Pasando a tensorflow form\ndataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))\n#dataset.take(2) # one sample data\n\ndef SentimentDatasetMapFunction(input_ids, attn_masks, labels):\n    return {\n        'input_ids': input_ids,\n        'attention_mask': attn_masks\n    }, labels\n\n### Preparación modelo\n# Seteando modelo\nprint(\"Batch size de:\",z,\"\\nNúmero de épocas de:\",epocas)\ndataset = dataset.map(SentimentDatasetMapFunction)\nprint(len(dataset))\ndataset = dataset.batch(z, drop_remainder=True)\nprint(len(dataset))\np = 0.8 # Porcentaje de datos para entrenar\ntrain_size = int((len(X_train)//z)*p)\ntrain_dataset = dataset.take(train_size)\nprint(len(train_dataset))\nval_dataset = dataset.skip(train_size)\nprint(len(val_dataset))\n\nfrom transformers import TFBertModel\nmodel = TFBertModel.from_pretrained(modelo)\n\n# Hay 2 inputs, los id y los attention mask\ninput_ids = tf.keras.layers.Input(shape=(256,), name='input_ids', dtype='int32')\nattn_masks = tf.keras.layers.Input(shape=(256,), name='attention_mask', dtype='int32')\n\n# Los embeddings\nbert_embds = model.bert(input_ids, attention_mask=attn_masks)[1] # 0 -> activation layer (3D), 1 -> pooled output layer (2D)\nintermediate_layer = tf.keras.layers.Dense(512, activation='relu', name='intermediate_layer')(bert_embds)\noutput_layer = tf.keras.layers.Dense(2, activation='softmax', name='output_layer')(intermediate_layer) # softmax -> calcs probs of classes\n\n# Arquitectura\nsentiment_model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)\nsentiment_model.summary()\n\n# Estimando modelo\noptim = tf.keras.optimizers.Adam(learning_rate=1e-5, decay=1e-6)\nloss_func = tf.keras.losses.CategoricalCrossentropy()\naccc = tf.keras.metrics.CategoricalAccuracy('accuracy')\nsentiment_model.compile(optimizer=optim, loss=loss_func, metrics=[accc])\nhist = sentiment_model.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=epocas\n)\n\n# Guardando\nsentiment_model.save(nombre_modelo)\n\n# Cargando\nsentiment_model = tf.keras.models.load_model(nombre_modelo)\ntokenizer = BertTokenizer.from_pretrained(modelo)\n\n### Evaluando lo obtenido\n# Funciones\ndef prepare_data(input_text, tokenizer):\n    token = tokenizer.encode_plus(\n        input_text,\n        max_length=256, \n        truncation=True, \n        padding='max_length', \n        add_special_tokens=True,\n        return_tensors='tf'\n    )\n    return {\n        'input_ids': tf.cast(token.input_ids, tf.float64),\n        'attention_mask': tf.cast(token.attention_mask, tf.float64)\n    }\n\ndef make_prediction(model, processed_data, classes=['Not_bullying', 'Bullying']):\n    probs = model.predict(processed_data)[0]\n    return classes[np.argmax(probs)]\n\n# Evaluando general\npd.DataFrame(hist.history).plot(figsize=(8,5))\nplt.grid(True)\nplt.gca().set_ylim(0,1)\nplt.show()\n\n# Evaluando específico\ninput_text = 'Parecía carbón'\nprocessed_data = prepare_data(input_text, tokenizer)\nresult = make_prediction(sentiment_model, processed_data=processed_data)\nprint(f\"Predicted Sentiment: {result}\")\n!zip -r /content/file.zip /content/bullying_beto_modelo\n\ny_pred = []\np=1\nfor i in X_test:\n  #print(p)\n  processed_data = prepare_data(i, tokenizer)\n  result = make_prediction(sentiment_model, processed_data=processed_data)\n  #print(f\"Predicted Sentiment: {result}\")\n  if result==\"Bullying\":\n    y_pred.append(1)\n  else:\n    y_pred.append(0)\n  p+=1\n\n# Para Excel\nmod.append(\"BETO\")\nf1.append(round(f1_score(y_test,y_pred, average='weighted'),2))\nacc.append(round(accuracy_score(y_test,y_pred),2))\nauc.append(round(roc_auc_score(y_test,y_pred),2))\nprec.append(round(precision_score(y_test,y_pred),2))\nprint(confusion_matrix(y_test, y_pred))\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\nspecificity = tn / (tn+fp)\nsensitivity = tp / (tp+fn)\nspec.append(round(specificity,2))\nsens.append(round(sensitivity,2))\nvpps = tp / (tp+fp)\nvpns = tn / (tn+fn)\nvpp.append(round(vpps,2))\nvpn.append(round(vpns,2))","metadata":{"id":"4FJCW1KsgCBj","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["c79a59f57ec74827ad6e4778b8bf486d","7e8dd1a0999a49c082fbbc3fe7d7cee1","3addb5f6e71a4449a80f44a9bd6c4cde","d98263853a754e79b63f427289c966bb","51d552a4dd7a4655a9fc47cbe2797cdc","cdcd3351670a4e4dbba4c4576d1b526b","88ac5007618347f691ca1393f94d9f3d","ffc818b7a77f4ec282869f7daab6b9cb","6f77949ccf5844b698abac193d37efbf","4e7141a9effd40419b3c0674fada151f","80ccba580ab7455588749b3f9ce8bd7b","5e8f0ca1e05440aa99e4acd83f1af943","8a66f54c8f31436ea2a9c44fffb9d227","f9d3168a0d71410d8403b2b0c5ea9f9d","ba126fd3a3454d7f86faa65f0b6e86c9","ead8cdc079c646738ead02a721e51eb8","d491b0f7bf9f44e99374b49a6642a445","fc784a2cb8d34dadbcf4259e53e9e1ae","4924a5cc4b764c178b0b5ea23f3667eb","7a50c38bd4c846c4b1e1331d7598a730","9ce7e041a27045c981d291cc3e55e404","c75a6f3f60c544668866d18ec16a79ec","2410203b98c64b98aff95c6ea67efd0c","9a35472029de463c9aec7cad4522caca","846894f1e09f4807819fa6eee5343d45","d6d1981d2512449995cdd3a01dcb5ba4","9da63116c8254ebdb946cff4a5799631","ddee723ca3364b5aa377da68c72e74fe","2a6f14b160ed465d8ce3f2eb0a81bc00","0def1b6230604f8e94a3ea3e6056a5fa","96cb8b4c75ee46a59df7ba8bea0e92fb","41948ac3572c45058d19a01ca05e7d09","c88e40e4524f43549b8d2bc2fdc4da6a","3c9451f9f3bd4927b3f23d9dc84e901f","a4bd592346984693834dd87d47835964","fc93d27b574547bf88412db399e4df7f","4f98c70406034557ac3bf46334739a19","a34209d0c913446881813f7858a0607f","2abf10ccc56e4577a1836f7b741b278d","965bf54deee0420fa1d8d0a3e5b477b1","8f513af1a1334a6497b2b54a16b0548e","e13a6c2179174497bed2cd44a98f8da3","6bf7da0fb34a4c98a8e95468f4708d79","e19b157d739942819bd177be0183ec44","f87ef69133204570a1f37b77a60ef1fa","d9429437ebc443aea678c77c687249b0","e1c24fc426764fabaf77563e53a3a559","0a75e07a74e644b09c614513f9fecabc","7f5bd67947f94cef94a3558a1cde5920","3ee3dabe34654d39ae093480c9073465","7dbdefad365843dbbdc834918c2a4223","86b04786d5d64145997b9a2bc4755160","7d229e3a54624f8396db310e4e84eede","42f7e96ecccd433098fc7586c90e0fd0","2531e11e03294f30a22107be4e4fc9ed","a41d7a5d0aa841808d29426f1e90e197","4abf629439d24e43bbadeb8bddd516b3","531d8569f3e2448580026c6e9b9459bd","e0f8c0cbc5f8411b8dbe8a2355c14cb8","6d9bf00ab51f413c9618f1b2af9b5c5e","17e98681acc14d22ae9cd2e9d9dbd187","ff7afdd3c5fd471ea4cc4d696de011f0","c446b9dae4834c0d919ee1a45196b2ae","5512625204554d6f8ebeb1e6d696c65e","9d664afada3444dcbd1d2b7f7a78b4cd","930f9acc52d348df8291679b9e4651bb"]},"outputId":"5e07b65b-3efb-4224-9e38-c97d5ea1ac32"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tutorial: https://www.youtube.com/watch?v=wp9BudYGZyA&t=1862s\n### Preparación general\n# Librerías para BETO\nfrom tqdm.auto import tqdm\nimport tensorflow as tf\nfrom transformers import BertTokenizer\nmodelo='dccuchile/bert-base-spanish-wwm-uncased'\nvariable_x=\"tweet_text_translated_clean\"\nnombre_modelo='bullying_beto_modelo'\nz=16 # Batch size\nepocas=3 # N° épocas\n\nX = dff[[\"Unnamed: 0\",'tweet_text','tweet_text_translated_clean','prediccion_jonatan']]\n# train-test-split para documents (textos sin íconos especiales ni doble espaciado)\nX_train, X_test, y_train, y_test = train_test_split(X[variable_x], y, test_size=0.2,random_state=17)\nprint(\"Hay por procesar:\", len(X_test))\nprint(X_test)\n\n# Trayendo al modelo de Hugging Face\ntokenizer = BertTokenizer.from_pretrained(modelo)\n\n# Tokenizando\ntoken = tokenizer.encode_plus(\n    X_train.iloc[0],\n    max_length=256, \n    truncation=True, \n    padding='max_length', \n    add_special_tokens=True,\n    return_tensors='tf'\n)\nprint(token.input_ids)\nX_input_ids = np.zeros((len(X_train), 256))\nX_attn_masks = np.zeros((len(X_train), 256))\n\n# Funciones\ndef generate_training_data(df, ids, masks, tokenizer):\n    for i, text in tqdm(enumerate(X_train)):\n        tokenized_text = tokenizer.encode_plus(\n            text,\n            max_length=256, \n            truncation=True, \n            padding='max_length', \n            add_special_tokens=True,\n            return_tensors='tf'\n        )\n        ids[i, :] = tokenized_text.input_ids\n        masks[i, :] = tokenized_text.attention_mask\n    return ids, masks\n\n# Creando efectivamente la tokenización\nX_input_ids, X_attn_masks = generate_training_data(X_train, X_input_ids, X_attn_masks, tokenizer)\nlabels = np.zeros((len(X_train), 2))\nlabels[np.arange(len(X_train)), y_train] = 1\n\n# Pasando a tensorflow form\ndataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))\n#dataset.take(2) # one sample data\n\ndef SentimentDatasetMapFunction(input_ids, attn_masks, labels):\n    return {\n        'input_ids': input_ids,\n        'attention_mask': attn_masks\n    }, labels\n\n### Preparación modelo\n# Seteando modelo\nprint(\"Batch size de:\",z,\"\\nNúmero de épocas de:\",epocas)\ndataset = dataset.map(SentimentDatasetMapFunction)\nprint(len(dataset))\ndataset = dataset.batch(z, drop_remainder=True)\nprint(len(dataset))\np = 0.8 # Porcentaje de datos para entrenar\ntrain_size = int((len(X_train)//z)*p)\ntrain_dataset = dataset.take(train_size)\nprint(len(train_dataset))\nval_dataset = dataset.skip(train_size)\nprint(len(val_dataset))\n\nfrom transformers import TFBertModel\nmodel = TFBertModel.from_pretrained(modelo)\n\n# Hay 2 inputs, los id y los attention mask\ninput_ids = tf.keras.layers.Input(shape=(256,), name='input_ids', dtype='int32')\nattn_masks = tf.keras.layers.Input(shape=(256,), name='attention_mask', dtype='int32')\n\n# Los embeddings\nbert_embds = model.bert(input_ids, attention_mask=attn_masks)[1] # 0 -> activation layer (3D), 1 -> pooled output layer (2D)\nintermediate_layer = tf.keras.layers.Dense(512, activation='relu', name='intermediate_layer')(bert_embds)\noutput_layer = tf.keras.layers.Dense(2, activation='softmax', name='output_layer')(intermediate_layer) # softmax -> calcs probs of classes\n\n# Arquitectura\nsentiment_model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)\nsentiment_model.summary()\n\n# Estimando modelo\noptim = tf.keras.optimizers.Adam(learning_rate=1e-5, decay=1e-6)\nloss_func = tf.keras.losses.CategoricalCrossentropy()\naccc = tf.keras.metrics.CategoricalAccuracy('accuracy')\nsentiment_model.compile(optimizer=optim, loss=loss_func, metrics=[accc])\nhist = sentiment_model.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=epocas\n)\n\n# Guardando\nsentiment_model.save(nombre_modelo)\n\n# Cargando\nsentiment_model = tf.keras.models.load_model(nombre_modelo)\ntokenizer = BertTokenizer.from_pretrained(modelo)\n\n### Evaluando lo obtenido\n# Funciones\ndef prepare_data(input_text, tokenizer):\n    token = tokenizer.encode_plus(\n        input_text,\n        max_length=256, \n        truncation=True, \n        padding='max_length', \n        add_special_tokens=True,\n        return_tensors='tf'\n    )\n    return {\n        'input_ids': tf.cast(token.input_ids, tf.float64),\n        'attention_mask': tf.cast(token.attention_mask, tf.float64)\n    }\n\ndef make_prediction(model, processed_data, classes=['Not_bullying', 'Bullying']):\n    probs = model.predict(processed_data)[0]\n    return classes[np.argmax(probs)]\n\n# Evaluando general\npd.DataFrame(hist.history).plot(figsize=(8,5))\nplt.grid(True)\nplt.gca().set_ylim(0,1)\nplt.show()\n\n# Evaluando específico\ninput_text = 'Parecía carbón'\nprocessed_data = prepare_data(input_text, tokenizer)\nresult = make_prediction(sentiment_model, processed_data=processed_data)\nprint(f\"Predicted Sentiment: {result}\")\n!zip -r /content/file.zip /content/bullying_beto_modelo\n\ny_pred = []\np=1\nfor i in X_test:\n  #print(p)\n  processed_data = prepare_data(i, tokenizer)\n  result = make_prediction(sentiment_model, processed_data=processed_data)\n  #print(f\"Predicted Sentiment: {result}\")\n  if result==\"Bullying\":\n    y_pred.append(1)\n  else:\n    y_pred.append(0)\n  p+=1\n\n# Para Excel\nmod.append(\"BETO\")\nf1.append(round(f1_score(y_test,y_pred, average='weighted'),2))\nacc.append(round(accuracy_score(y_test,y_pred),2))\nauc.append(round(roc_auc_score(y_test,y_pred),2))\nprec.append(round(precision_score(y_test,y_pred),2))\nprint(confusion_matrix(y_test, y_pred))\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\nspecificity = tn / (tn+fp)\nsensitivity = tp / (tp+fn)\nspec.append(round(specificity,2))\nsens.append(round(sensitivity,2))\nvpps = tp / (tp+fp)\nvpns = tn / (tn+fn)\nvpp.append(round(vpps,2))\nvpn.append(round(vpns,2))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["53a1f2a83f764b5ca802623bb42daa42","7c038ffdefb84a97a6de0751cb0dbb85","8932df5caa77418d9cf35610d98a898c","e29718048e094feeb3d0b180df108ce4","4ad5c164194348e48a5a5aca97fc2314","163034a7e1f1409b94969d6e50c6495e","e040abd257d34e1bb8019c58fa554879","a3da567311294763a912d18b0b59ff72","c0897fbc389543eb818e4ecd45c743b1","b54daffd36c14a6d94db71c20561e73b","1a9039a1771249998e1cc39c644f3de9"]},"id":"4YnkFKdUAUXd","outputId":"89d4cd9a-657f-455c-9b4f-5fec9d02f761"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Comparativa","metadata":{"id":"FrnC754vDR0J"}},{"cell_type":"code","source":"datax = list(zip(mod,acc,prec,spec,sens,f1,auc,vpn))\ncolumns = [\"Modelo\",\"Accuracy\",\"Precision\",\"Specificity\",\"Sensivility\",\"F1-score\",\"AUC\",\"FPV\"]\ndatax = pd.DataFrame(datax,columns=columns)\ndatax.to_excel(\"aqui.xlsx\",index=False)","metadata":{"id":"LP0CFwU4DSKK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Calibración","metadata":{"id":"laah8taIAUV-"}},{"cell_type":"code","source":"plt.clf()\n\n# Calibración\nprint(nn.get_params())\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {'learning_rate_init': [0.1, 0.3, 0.5, 0.7], 'solver': ['sgd', 'adam'], 'hidden_layer_sizes': [(150,100,50,10),(100,50,10)]}\n\nscoring = ['f1_macro']\nlearning_rate=[0.1, 0.3, 0.5, 0.7]\nsolver = ['sgd', 'adam']\nhls= [(150,100,50,10),(100,50,10)]\n\nlr=[]\nso=[]\nhl=[]\nf1=[]\nacc=[]\nauc=[]\nprec=[]\nspec=[]\nsens=[]\nvpp=[]\nvpn=[]\n\nfor i in learning_rate:\n  for j in solver:\n    for k in hls:\n      print(\"Probando con lr de\", str(i), \", un solver de\", str(j),\" y un hls de\", k)\n      model=MLPClassifier(hidden_layer_sizes=k, max_iter=300, learning_rate_init=i, activation = 'relu', solver=j,random_state=17).fit(X_train,y_train)\n      y_pred = model.predict(X_test)\n      lr.append(i)\n      f1.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n      acc.append(round(accuracy_score(y_test,y_pred),2))\n      auc.append(round(roc_auc_score(y_test,y_pred),2))\n      prec.append(round(precision_score(y_test,y_pred),2))\n      tn_s, fp_s, fn_s, tp_s = confusion_matrix(y_test, y_pred).ravel()\n      specificity_s = tn_s / (tn_s+fp_s)\n      sensitivity_s = tp_s / (tp_s+fn_s)\n      spec.append(round(specificity_s,2))\n      sens.append(round(sensitivity_s,2))\n      vpps_s = tp_s / (tp_s+fp_s)\n      vpns_s = tn_s / (tn_s+fn_s)\n      vpp.append(round(vpps_s,2))\n      vpn.append(round(vpns_s,2))\n\ndatax = list(zip(lr,acc,prec,spec,sens,f1,auc,vpn))\ncolumns = [\"Learning rate\",\"Accuracy\",\"Precision\",\"Specificity\",\"Sensivility\",\"F1-score\",\"AUC\",\"NPV\"]\ndatax = pd.DataFrame(datax,columns=columns)\ndatax.to_excel(\"aqui2.xlsx\",index=False)","metadata":{"id":"LJR5Jjc_AWtG","colab":{"base_uri":"https://localhost:8080/","height":662},"outputId":"3b8fccd4-c76c-4d22-dd67-0d7ad80a9619"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Inglés","metadata":{"id":"WdjhtsYN_9N4"}},{"cell_type":"markdown","source":"### Estadística descriptiva","metadata":{"id":"arHNZrAHysyS"}},{"cell_type":"code","source":"# Modelando\nf1=[]\nacc=[]\nprec=[]\nspec=[]\nsens=[]\nvpp=[]\nvpn=[]\nauc=[]\nmod=[]\ndat=[]\n\n# Longitud\ntext_len = []\nfor text in dff[\"tweet_text\"]:\n    tweet_len = len(text.split())\n    text_len.append(tweet_len)\n\ndff['tweet_text_len'] = text_len\nplt.figure(figsize=(7,5))\nax = sns.countplot(x='tweet_text_len', data=dff, palette='magma')\nplt.title('Conteo de textos originales por número de palabras (inglés)', fontsize=20)\nplt.yticks([])\nplt.xticks([])\n#for i in ax.containers:\n#    ax.bar_label(i,)\nplt.ylabel('conteo')\nplt.xlabel('')\nplt.show()\nprint(dff['tweet_text_len'].describe())\n\nfrom collections import Counter\nresults = set()\ndff['tweet_text'].str.lower().str.split().apply(results.update)\nresults = Counter()\ndff['tweet_text'].str.lower().str.split().apply(results.update)\nprint(len(results))\nprint(results.most_common())","metadata":{"id":"UqhgFE_mysYy","colab":{"base_uri":"https://localhost:8080/","height":539},"outputId":"412a6dd9-0514-4ccb-817e-176afbefd432"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TF-IDF","metadata":{"id":"fEePeu9f_9OA"}},{"cell_type":"code","source":"nlp = en_core_web_md.load()\ndocuments = []\nj = 0\nX = dff[[\"Unnamed: 0\",'tweet_text','tweet_text_translated','prediccion_jonatan']]\nXX = X[\"tweet_text\"]\ny = yyy[\"cyberbullying_type2\"]\n\nif pp!=1:\n  for sen in XX.index:\n      # Removiendo caracteres especiales\n      #print(X[sen])\n      document = re.sub(r'\\W', ' ', str(XX[sen]))\n      \n      # Removiendo caracteres solitarios despues del inicio\n      document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n      \n      # Removiendo caracteres solitarios al inicio\n      document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n\n      # Removiendo multiples espacios\n      document = re.sub(r'\\s+', ' ', document, flags=re.I)\n\n      # Removiendo el prefijo b\n      document = re.sub(r'^b\\s+', '', document) #solo cuando se agrega al inicio\n      \n      # Minúsculas\n      document = document.lower()\n\n      # Removiendo caracteres solitarios al inicio\n      document = re.sub('rt', '', document) \n\n      # Removiendo caracteres solitarios al inicio\n      document = re.sub('http', '', document)\n\n      # Removiendo caracteres solitarios al inicio\n      document = re.sub('él', '', document)\n\n      # Removiendo caracteres solitarios al inicio\n      document = re.sub('co', '', document)\n      \n      # Lematizacion\n      document = nlp (document)\n      lemmas = [no_stop.lemma_ for no_stop in document if not no_stop.is_stop]\n      document = ' '.join(lemmas)\n      documents.append(document)\n\n      # Removiendo caracteres solitarios al inicio\n      document = re.sub('él', '', document)\n\n      # Removiendo caracteres solitarios despues del inicio\n      document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n      \n      # Removiendo caracteres solitarios al inicio\n      document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n\n      j = j+1\n      print(j)\n\n  # Longitud\n  text_len = []\n  for text in documents:\n      tweet_len = len(text.split())\n      text_len.append(tweet_len)\n\n  dff[\"tweet_text_clean\"]=documents\n  dff['text_clean_len'] = text_len\n  dff.to_excel(\"data_lista.xlsx\",index=False)\n\nplt.figure(figsize=(7,5))\nax = sns.countplot(x='text_translated_clean_len', data=dff, palette='magma')\nplt.title('Conteo de textos limpiados por número de palabras (inglés)', fontsize=20)\nplt.yticks([])\nplt.xticks([])\n#for i in ax.containers:\n#    ax.bar_label(i,)\nplt.ylabel('conteo')\nplt.xlabel('')\nplt.show()\nprint(dff['text_clean_len'].describe())\n\nfrom collections import Counter\nresults = set()\ndff['tweet_text_clean'].str.lower().str.split().apply(results.update)\nresults = Counter()\ndff['tweet_text_clean'].str.lower().str.split().apply(results.update)\nprint(len(results))\nprint(results.most_common()[0:10])\nyy = [count for tag, count in results.most_common(20)]\nx = [tag for tag, count in results.most_common(20)]\n\nplt.barh(x,yy, color='crimson')\n\n# Vectorizar\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(max_features=1000, min_df=5, max_df=0.7)\nXX = vectorizer.fit_transform(dff['tweet_text_clean']).toarray()\n\n# TF-IDF\nfrom sklearn.feature_extraction.text import TfidfTransformer\ntfidfconverter = TfidfTransformer()\nXX = tfidfconverter.fit_transform(XX).toarray()\n\n# Separando (equivalente al de arriba por el random state)\nX_train, X_test, y_train, y_test = train_test_split(XX, y, test_size=0.2,random_state=17)\nprint(\"Hay por procesar:\", len(X_test))\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":784},"id":"M4Qc0HMj_9OA","outputId":"ff4e56c4-4a91-4b90-8a84-c39e5eaa69a7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  + RF","metadata":{"id":"msUNjZjz_9OA"}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=1000, random_state=0)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\n\n# Escribe tu propia reseña y prueba tu modelo\nreview1 = \"Perra loca\"\nreview1_features = get_features(review1)\nprint(\"Review:\", review1)\nprint(\"Probabilidad de bullying:\", rf.predict_proba(review1_features)[0,1]) #0 es por la primera obs, y 1 es por prob de posi\n\nmod.append(\"TF-IDF + RF\")\nf1.append(round(f1_score(y_test,y_pred, average='weighted'),2))\nacc.append(round(accuracy_score(y_test,y_pred),2))\nauc.append(round(roc_auc_score(y_test,y_pred),2))\nprec.append(round(precision_score(y_test,y_pred),2))\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\nspecificity = tn / (tn+fp)\nsensitivity = tp / (tp+fn)\nspec.append(round(specificity,2))\nsens.append(round(sensitivity,2))\nvpps = tp / (tp+fp)\nvpns = tn / (tn+fn)\nvpp.append(round(vpps,2))\nvpn.append(round(vpns,2))","metadata":{"id":"7QdmAvT4_9OA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5f178741-658b-4c83-823c-ab27845df521"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### + SVM","metadata":{"id":"nc7QYj39_9OB"}},{"cell_type":"code","source":"from sklearn.svm import SVC\nsv = SVC(random_state=0)\nsv.fit(X_train, y_train)\ny_pred = sv.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\n\n# Escribe tu propia reseña y prueba tu modelo\nreview1 = \"Perra loca\"\nreview1_features = get_features(review1)\nprint(\"Review:\", review1)\nprint(\"Probabilidad de bullying:\", sv.predict(review1_features.toarray())) #0 es por la primera obs, y 1 es por prob de posi\n\nmod.append(\"TF-IDF + SVM\")\nf1.append(round(f1_score(y_test,y_pred, average='weighted'),2))\nacc.append(round(accuracy_score(y_test,y_pred),2))\nauc.append(round(roc_auc_score(y_test,y_pred),2))\nprec.append(round(precision_score(y_test,y_pred),2))\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\nspecificity = tn / (tn+fp)\nsensitivity = tp / (tp+fn)\nspec.append(round(specificity,2))\nsens.append(round(sensitivity,2))\nvpps = tp / (tp+fp)\nvpns = tn / (tn+fn)\nvpp.append(round(vpps,2))\nvpn.append(round(vpns,2))","metadata":{"id":"LG-Gcq45_9OB","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9d1ec492-fb94-4b82-a817-d953c592c4c7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### + Naive Bayes","metadata":{"id":"CjqEQyet_9OB"}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(X_train, y_train)\ny_pred = nb.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\n\n# Escribe tu propia reseña y prueba tu modelo\nreview1 = \"Puta\"\nreview1_features = get_features(review1)\nprint(\"Review:\", review1)\nprint(\"Probabilidad de bullying:\", nb.predict_proba(review1_features.toarray())[0,1]) #0 es por la primera obs, y 1 es por prob de posi\n\nmod.append(\"TF-IDF + Naive Bayes\")\nf1.append(round(f1_score(y_test,y_pred, average='weighted'),2))\nacc.append(round(accuracy_score(y_test,y_pred),2))\nauc.append(round(roc_auc_score(y_test,y_pred),2))\nprec.append(round(precision_score(y_test,y_pred),2))\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\nspecificity = tn / (tn+fp)\nsensitivity = tp / (tp+fn)\nspec.append(round(specificity,2))\nsens.append(round(sensitivity,2))\nvpps = tp / (tp+fp)\nvpns = tn / (tn+fn)\nvpp.append(round(vpps,2))\nvpn.append(round(vpns,2))","metadata":{"id":"pGzpGq0j_9OB","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ca55e869-27d9-47d9-b3f1-388e786e0e9e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### + Redes neuronales","metadata":{"id":"xB23VMK0_9OC"}},{"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nnn = MLPClassifier(hidden_layer_sizes=(150,100,50,10), max_iter=300, activation = 'relu',solver='sgd',random_state=17)\nnn.fit(X_train, y_train)\ny_pred = nn.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\n\n# Escribe tu propia reseña y prueba tu modelo\nreview1 = \"Puta\"\nreview1_features = get_features(review1)\nprint(\"Review:\", review1)\nprint(\"Probabilidad de bullying:\", nn.predict_proba(review1_features.toarray())[0,1]) #0 es por la primera obs, y 1 es por prob de posi\n\nmod.append(\"TF-IDF + Redes Neuronales\")\nf1.append(round(f1_score(y_test,y_pred, average='weighted'),2))\nacc.append(round(accuracy_score(y_test,y_pred),2))\nauc.append(round(roc_auc_score(y_test,y_pred),2))\nprec.append(round(precision_score(y_test,y_pred),2))\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\nspecificity = tn / (tn+fp)\nsensitivity = tp / (tp+fn)\nspec.append(round(specificity,2))\nsens.append(round(sensitivity,2))\nvpps = tp / (tp+fp)\nvpns = tn / (tn+fn)\nvpp.append(round(vpps,2))\nvpn.append(round(vpns,2))","metadata":{"id":"rD8quCYT_9OC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"79571715-6c4c-4fea-f75f-4561bfd9d001"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### BERT","metadata":{"id":"4McEXd6wbIg9"}},{"cell_type":"code","source":"# Tutorial: https://www.youtube.com/watch?v=wp9BudYGZyA&t=1862s\n### Preparación general\n# Librerías para BERT\nfrom tqdm.auto import tqdm\nimport tensorflow as tf\nfrom transformers import BertTokenizer\nmodelo='bert-base-cased'\nvariable_x=\"tweet_text\"\nnombre_modelo='bullying_bert_model'\nz=16 # Batch size\nepocas=3 # N° épocas\n\nX = dff[[\"Unnamed: 0\",'tweet_text','tweet_text_translated','prediccion_jonatan']]\n# train-test-split para documents (textos sin íconos especiales ni doble espaciado)\nX_train, X_test, y_train, y_test = train_test_split(X[variable_x], y, test_size=0.2,random_state=17)\nprint(\"Hay por procesar:\", len(X_test))\nprint(X_test)\n\n# Trayendo al modelo de Hugging Face\ntokenizer = BertTokenizer.from_pretrained(modelo)\n\n# Tokenizando\ntoken = tokenizer.encode_plus(\n    X_train.iloc[0],\n    max_length=256, \n    truncation=True, \n    padding='max_length', \n    add_special_tokens=True,\n    return_tensors='tf'\n)\nprint(token.input_ids)\nX_input_ids = np.zeros((len(X_train), 256))\nX_attn_masks = np.zeros((len(X_train), 256))\n\n# Funciones\ndef generate_training_data(df, ids, masks, tokenizer):\n    for i, text in tqdm(enumerate(X_train)):\n        tokenized_text = tokenizer.encode_plus(\n            text,\n            max_length=256, \n            truncation=True, \n            padding='max_length', \n            add_special_tokens=True,\n            return_tensors='tf'\n        )\n        ids[i, :] = tokenized_text.input_ids\n        masks[i, :] = tokenized_text.attention_mask\n    return ids, masks\n\n# Creando efectivamente la tokenización\nX_input_ids, X_attn_masks = generate_training_data(X_train, X_input_ids, X_attn_masks, tokenizer)\nlabels = np.zeros((len(X_train), 2))\nlabels[np.arange(len(X_train)), y_train] = 1\n\n# Pasando a tensorflow form\ndataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))\n#dataset.take(2) # one sample data\n\ndef SentimentDatasetMapFunction(input_ids, attn_masks, labels):\n    return {\n        'input_ids': input_ids,\n        'attention_mask': attn_masks\n    }, labels\n\n### Preparación modelo\n# Seteando modelo\nprint(\"Batch size de:\",z,\"\\nNúmero de épocas de:\",epocas)\ndataset = dataset.map(SentimentDatasetMapFunction)\nprint(len(dataset))\ndataset = dataset.batch(z, drop_remainder=True)\nprint(len(dataset))\np = 0.8 # Porcentaje de datos para entrenar\ntrain_size = int((len(X_train)//z)*p)\ntrain_dataset = dataset.take(train_size)\nprint(len(train_dataset))\nval_dataset = dataset.skip(train_size)\nprint(len(val_dataset))\n\nfrom transformers import TFBertModel\nmodel = TFBertModel.from_pretrained(modelo)\n\n# Hay 2 inputs, los id y los attention mask\ninput_ids = tf.keras.layers.Input(shape=(256,), name='input_ids', dtype='int32')\nattn_masks = tf.keras.layers.Input(shape=(256,), name='attention_mask', dtype='int32')\n\n# Los embeddings\nbert_embds = model.bert(input_ids, attention_mask=attn_masks)[1] # 0 -> activation layer (3D), 1 -> pooled output layer (2D)\nintermediate_layer = tf.keras.layers.Dense(512, activation='relu', name='intermediate_layer')(bert_embds)\noutput_layer = tf.keras.layers.Dense(2, activation='softmax', name='output_layer')(intermediate_layer) # softmax -> calcs probs of classes\n\n# Arquitectura\nsentiment_model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)\nsentiment_model.summary()\n\n# Estimando modelo\noptim = tf.keras.optimizers.Adam(learning_rate=1e-5, decay=1e-6)\nloss_func = tf.keras.losses.CategoricalCrossentropy()\naccc = tf.keras.metrics.CategoricalAccuracy('accuracy')\nsentiment_model.compile(optimizer=optim, loss=loss_func, metrics=[accc])\nhist = sentiment_model.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=epocas\n)\n\n# Guardando\nsentiment_model.save(nombre_modelo)\n\n# Cargando\nsentiment_model = tf.keras.models.load_model(nombre_modelo)\ntokenizer = BertTokenizer.from_pretrained(modelo)\n\n### Evaluando lo obtenido\n# Funciones\ndef prepare_data(input_text, tokenizer):\n    token = tokenizer.encode_plus(\n        input_text,\n        max_length=256, \n        truncation=True, \n        padding='max_length', \n        add_special_tokens=True,\n        return_tensors='tf'\n    )\n    return {\n        'input_ids': tf.cast(token.input_ids, tf.float64),\n        'attention_mask': tf.cast(token.attention_mask, tf.float64)\n    }\n\ndef make_prediction(model, processed_data, classes=['Not_bullying', 'Bullying']):\n    probs = model.predict(processed_data)[0]\n    return classes[np.argmax(probs)]\n\n# Evaluando general\npd.DataFrame(hist.history).plot(figsize=(8,5))\nplt.grid(True)\nplt.gca().set_ylim(0,1)\nplt.show()\n\n# Evaluando específico\ninput_text = 'You look like shit'\nprocessed_data = prepare_data(input_text, tokenizer)\nresult = make_prediction(sentiment_model, processed_data=processed_data)\nprint(f\"Predicted Sentiment: {result}\")\n!zip -r /content/file.zip /content/bullying_beto_modelo\n\ny_pred = []\np=1\nfor i in X_test:\n  #print(p)\n  processed_data = prepare_data(i, tokenizer)\n  result = make_prediction(sentiment_model, processed_data=processed_data)\n  #print(f\"Predicted Sentiment: {result}\")\n  if result==\"Bullying\":\n    y_pred.append(1)\n  else:\n    y_pred.append(0)\n  p+=1\n\n# Para Excel\nmod.append(\"BERT\")\nf1.append(round(f1_score(y_test,y_pred, average='weighted'),2))\nacc.append(round(accuracy_score(y_test,y_pred),2))\nauc.append(round(roc_auc_score(y_test,y_pred),2))\nprec.append(round(precision_score(y_test,y_pred),2))\nprint(confusion_matrix(y_test, y_pred))\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\nspecificity = tn / (tn+fp)\nsensitivity = tp / (tp+fn)\nspec.append(round(specificity,2))\nsens.append(round(sensitivity,2))\nvpps = tp / (tp+fp)\nvpns = tn / (tn+fn)\nvpp.append(round(vpps,2))\nvpn.append(round(vpns,2))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["90de54d8c84b483abc4fda547dd74eee","394ac5f8d6c24c5eb5b9d4d408d7aa60","0ee04c1d20c2455ebe1bd3ed2f9cb2c6","c45691e35bca445f8bdfe439b78c313e","c63a99685e3a46ab881fd28e5ddc7215","f58514e29e1d4875ad8e4d86bd179be0","7556e16154c34b418a85ae8a2223a921","f20cbf34541c4bbb94c3c0415838cff5","b0296bd79e234a33a6276305e38fc03c","0b1cd050477046dfa4815e3903abf4bd","bbeb92617d4843738e3001ae6402833d","690f7105ee2a49a7a95aa4ffcf9fc924","26bdd9977f9749b69c2c66b9642fdf76","0453506d837c4f8db3d7f10366407608","3ab3eaf5ebf746339841776cea9ce760","87bd52749402444cb0f37be7894d6c53","8956647344c443e981fac4f67c7dad97","4bcf5dc621d747cc8623148fb05e6dac","78f03204d8194c0dbfeef04aa56e21dd","5680145429024b298711c4b376702091","f66e4273d0bd45d6999c92c68d274e9b","55cd0a51c6754d90b1626250d0c98dc3","917cebfbfa674cb39fe215f12fdf3cc1","e86c4c565e12461cbdbc0a206e350b7a","ee607256762a44fa9eb13352ab6632bb","586cecfbc4094fc0b2fe19572db86984","b8afb40c835942ed8725430344524baf","03b3d80b595841f1923834b9111934cd","54f2117438f64091be0d740e6448d5b5","46b10644ca03466da3caeb2d58188a41","d3934bcb57fb48b4b1063926240da9d4","c1c5a0e07b0c46ed9923c1f094391c4f","d778019c308f4bb888c9e11296db1dd5","c0cdadd2914c42c2b74cd4cee87bca39","24cee8a9b4d94340b6069f4bd3fd95a6","992e0de953684e249ac8ead9f5624025","61f357fefed8478fb45348a5af31c6d9","d7fc855100144198a732c3f2920a18ab","b34bbdacac004775a3c8a53af9d23f21","74c58560d40d44408c71b3429146434c","296613651f8b4867a48ead41657e9f33","409d3de5ea0b4b9dabbb792e11d3579c","d6c669cc71504b79a3352bf7f892c114","4875ae98ac10423cb215586feaa60b0b","ad70ad6b60c14316ad58a320e31ac10e","02305d605bde4e85bb13db9be23ed7dd","100c5d1be6e24e8bb8462efa2431db10","0ec87ad75470447d89287ac45b572ff5","03303dcfe6984936b49e16f7cda89e94","df50db067e864c99b1c1091c670e56d5","93f7ff58d5ea489abe21e97a75a9e9b1","26c46ba02357475092a8576c8901738f","3f9671cc6d174eebad0bac9703a1e50f","77491e77b99e4214b941682fec554729","e14a1c787e694f80a4825e0bc3438279"]},"id":"K8spn-9FbI4o","outputId":"1d9e00d7-699c-4cf9-88a6-d3165a6ac8f3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tutorial: https://www.youtube.com/watch?v=wp9BudYGZyA&t=1862s\n### Preparación general\n# Librerías para BERT\nfrom tqdm.auto import tqdm\nimport tensorflow as tf\nfrom transformers import BertTokenizer\nmodelo='bert-base-cased'\nvariable_x=\"tweet_text_clean\"\nnombre_modelo='bullying_bert_model'\nz=16 # Batch size\nepocas=3 # N° épocas\n\nX = dff[[\"Unnamed: 0\",'tweet_text_clean','tweet_text_translated','prediccion_jonatan']]\n# train-test-split para documents (textos sin íconos especiales ni doble espaciado)\nX_train, X_test, y_train, y_test = train_test_split(X[variable_x], y, test_size=0.2,random_state=17)\nprint(\"Hay por procesar:\", len(X_test))\nprint(X_test)\n\n# Trayendo al modelo de Hugging Face\ntokenizer = BertTokenizer.from_pretrained(modelo)\n\n# Tokenizando\ntoken = tokenizer.encode_plus(\n    X_train.iloc[0],\n    max_length=256, \n    truncation=True, \n    padding='max_length', \n    add_special_tokens=True,\n    return_tensors='tf'\n)\nprint(token.input_ids)\nX_input_ids = np.zeros((len(X_train), 256))\nX_attn_masks = np.zeros((len(X_train), 256))\n\n# Funciones\ndef generate_training_data(df, ids, masks, tokenizer):\n    for i, text in tqdm(enumerate(X_train)):\n        tokenized_text = tokenizer.encode_plus(\n            text,\n            max_length=256, \n            truncation=True, \n            padding='max_length', \n            add_special_tokens=True,\n            return_tensors='tf'\n        )\n        ids[i, :] = tokenized_text.input_ids\n        masks[i, :] = tokenized_text.attention_mask\n    return ids, masks\n\n# Creando efectivamente la tokenización\nX_input_ids, X_attn_masks = generate_training_data(X_train, X_input_ids, X_attn_masks, tokenizer)\nlabels = np.zeros((len(X_train), 2))\nlabels[np.arange(len(X_train)), y_train] = 1\n\n# Pasando a tensorflow form\ndataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))\n#dataset.take(2) # one sample data\n\ndef SentimentDatasetMapFunction(input_ids, attn_masks, labels):\n    return {\n        'input_ids': input_ids,\n        'attention_mask': attn_masks\n    }, labels\n\n### Preparación modelo\n# Seteando modelo\nprint(\"Batch size de:\",z,\"\\nNúmero de épocas de:\",epocas)\ndataset = dataset.map(SentimentDatasetMapFunction)\nprint(len(dataset))\ndataset = dataset.batch(z, drop_remainder=True)\nprint(len(dataset))\np = 0.8 # Porcentaje de datos para entrenar\ntrain_size = int((len(X_train)//z)*p)\ntrain_dataset = dataset.take(train_size)\nprint(len(train_dataset))\nval_dataset = dataset.skip(train_size)\nprint(len(val_dataset))\n\nfrom transformers import TFBertModel\nmodel = TFBertModel.from_pretrained(modelo)\n\n# Hay 2 inputs, los id y los attention mask\ninput_ids = tf.keras.layers.Input(shape=(256,), name='input_ids', dtype='int32')\nattn_masks = tf.keras.layers.Input(shape=(256,), name='attention_mask', dtype='int32')\n\n# Los embeddings\nbert_embds = model.bert(input_ids, attention_mask=attn_masks)[1] # 0 -> activation layer (3D), 1 -> pooled output layer (2D)\nintermediate_layer = tf.keras.layers.Dense(512, activation='relu', name='intermediate_layer')(bert_embds)\noutput_layer = tf.keras.layers.Dense(2, activation='softmax', name='output_layer')(intermediate_layer) # softmax -> calcs probs of classes\n\n# Arquitectura\nsentiment_model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)\nsentiment_model.summary()\n\n# Estimando modelo\noptim = tf.keras.optimizers.Adam(learning_rate=1e-5, decay=1e-6)\nloss_func = tf.keras.losses.CategoricalCrossentropy()\naccc = tf.keras.metrics.CategoricalAccuracy('accuracy')\nsentiment_model.compile(optimizer=optim, loss=loss_func, metrics=[accc])\nhist = sentiment_model.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=epocas\n)\n\n# Guardando\nsentiment_model.save(nombre_modelo)\n\n# Cargando\nsentiment_model = tf.keras.models.load_model(nombre_modelo)\ntokenizer = BertTokenizer.from_pretrained(modelo)\n\n### Evaluando lo obtenido\n# Funciones\ndef prepare_data(input_text, tokenizer):\n    token = tokenizer.encode_plus(\n        input_text,\n        max_length=256, \n        truncation=True, \n        padding='max_length', \n        add_special_tokens=True,\n        return_tensors='tf'\n    )\n    return {\n        'input_ids': tf.cast(token.input_ids, tf.float64),\n        'attention_mask': tf.cast(token.attention_mask, tf.float64)\n    }\n\ndef make_prediction(model, processed_data, classes=['Not_bullying', 'Bullying']):\n    probs = model.predict(processed_data)[0]\n    return classes[np.argmax(probs)]\n\n# Evaluando general\npd.DataFrame(hist.history).plot(figsize=(8,5))\nplt.grid(True)\nplt.gca().set_ylim(0,1)\nplt.show()\n\n# Evaluando específico\ninput_text = 'You look like shit'\nprocessed_data = prepare_data(input_text, tokenizer)\nresult = make_prediction(sentiment_model, processed_data=processed_data)\nprint(f\"Predicted Sentiment: {result}\")\n!zip -r /content/file.zip /content/bullying_beto_modelo\n\ny_pred = []\np=1\nfor i in X_test:\n  #print(p)\n  processed_data = prepare_data(i, tokenizer)\n  result = make_prediction(sentiment_model, processed_data=processed_data)\n  #print(f\"Predicted Sentiment: {result}\")\n  if result==\"Bullying\":\n    y_pred.append(1)\n  else:\n    y_pred.append(0)\n  p+=1\n\n# Para Excel\nmod.append(\"BERT\")\nf1.append(round(f1_score(y_test,y_pred, average='weighted'),2))\nacc.append(round(accuracy_score(y_test,y_pred),2))\nauc.append(round(roc_auc_score(y_test,y_pred),2))\nprec.append(round(precision_score(y_test,y_pred),2))\nprint(confusion_matrix(y_test, y_pred))\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\nspecificity = tn / (tn+fp)\nsensitivity = tp / (tp+fn)\nspec.append(round(specificity,2))\nsens.append(round(sensitivity,2))\nvpps = tp / (tp+fp)\nvpns = tn / (tn+fn)\nvpp.append(round(vpps,2))\nvpn.append(round(vpns,2))","metadata":{"id":"8uR7E66cKxg2","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["d890551714f746399289baa0efaeeed7","1bcb84783315430fb3b11df19b0852b9","f531b7dae0824d0f887e42a694a5cbdf","3a8434e1d6924e69add602cee5fde465","8452c24c77284b4f824012feaa7974a0","ac5586cbcced41e5bc2e93c5d92ada82","e2c9bfd614b34ba2a18ad720a179f3f3","fe9754e08dd54c6eb70c562cce02cbae","bbaf9f2e6b1a492da29d1bf8c8236f89","e2913a51ddb34f318d2bbb4a3daf5cdf","dc8c2e31f4c54cd08a28faee22da8742"]},"outputId":"07c67666-9697-4cc3-a95f-73b1acb987e0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Comparativa","metadata":{"id":"UC9tzn69TY0E"}},{"cell_type":"code","source":"datax = list(zip(mod,acc,prec,spec,sens,f1,auc,vpn))\ncolumns = [\"Modelo\",\"Accuracy\",\"Precision\",\"Specificity\",\"Sensivility\",\"F1-score\",\"AUC\",\"FPV\"]\ndatax = pd.DataFrame(datax,columns=columns)\ndatax.to_excel(\"aqui.xlsx\",index=False)\n","metadata":{"id":"vgakbd4hTZGc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Calibración","metadata":{"id":"dcI7_Eh-UWKH"}},{"cell_type":"code","source":"plt.clf()\n\n# Calibración\nprint(nn.get_params())\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {'learning_rate_init': [0.1, 0.3, 0.5, 0.7], 'solver': ['sgd', 'adam'], 'hidden_layer_sizes': [(150,100,50,10),(100,50,10)]}\n\nscoring = ['f1_macro']\nlearning_rate=[0.1, 0.3, 0.5, 0.7]\nsolver = ['sgd', 'adam']\nhls= [(150,100,50,10),(100,50,10)]\n\nlr=[]\nso=[]\nhl=[]\nf1=[]\nacc=[]\nauc=[]\nprec=[]\nspec=[]\nsens=[]\nvpp=[]\nvpn=[]\n\nfor i in learning_rate:\n  for j in solver:\n    for k in hls:\n      print(\"Probando con lr de\", str(i), \", un solver de\", str(j),\" y un hls de\", k)\n      model=MLPClassifier(hidden_layer_sizes=k, max_iter=300, learning_rate_init=i, activation = 'relu', solver=j,random_state=17).fit(X_train,y_train)\n      y_pred = model.predict(X_test)\n      lr.append(i)\n      f1.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n      acc.append(round(accuracy_score(y_test,y_pred),2))\n      auc.append(round(roc_auc_score(y_test,y_pred),2))\n      prec.append(round(precision_score(y_test,y_pred),2))\n      tn_s, fp_s, fn_s, tp_s = confusion_matrix(y_test, y_pred).ravel()\n      specificity_s = tn_s / (tn_s+fp_s)\n      sensitivity_s = tp_s / (tp_s+fn_s)\n      spec.append(round(specificity_s,2))\n      sens.append(round(sensitivity_s,2))\n      vpps_s = tp_s / (tp_s+fp_s)\n      vpns_s = tn_s / (tn_s+fn_s)\n      vpp.append(round(vpps_s,2))\n      vpn.append(round(vpns_s,2))\n\ndatax = list(zip(lr,acc,prec,spec,sens,f1,auc,vpn))\ncolumns = [\"Learning rate\",\"Accuracy\",\"Precision\",\"Specificity\",\"Sensivility\",\"F1-score\",\"AUC\",\"NPV\"]\ndatax = pd.DataFrame(datax,columns=columns)\ndatax.to_excel(\"aqui2.xlsx\",index=False)","metadata":{"id":"qLOip5oaUWc1","colab":{"base_uri":"https://localhost:8080/","height":662},"outputId":"c6d2c131-16af-4234-eefe-d10cee90d4d1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Multi clase","metadata":{"id":"AkHJ0ovQpYOL"}},{"cell_type":"markdown","source":"# Español","metadata":{"id":"OyT3JB8fpaLs"}},{"cell_type":"markdown","source":"## Estadística descriptiva","metadata":{"id":"43QjVwFwZcRm"}},{"cell_type":"code","source":"# Modelando\nf1=[]\nacc=[]\nprec=[]\nspec=[]\nsens=[]\nvpp=[]\nvpn=[]\nauc=[]\nmod=[]\ndat=[]\n\netiquetas=[\"No\",\"Edad\",\"Género\",\"Etnicidad\",\"Religión\",\"Otro tipo\"]","metadata":{"id":"agX51xm6Zcmc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TF-IDF","metadata":{"id":"JargLFiWYt1d"}},{"cell_type":"code","source":"nlp = es_core_news_md.load()\ndocuments = []\nj = 0\nX = dff[[\"Unnamed: 0\",'tweet_text','tweet_text_translated','prediccion_jonatan','tweet_text_translated_clean']]\nXX = X[\"tweet_text_translated\"]\ny = yyy[\"cyberbullying_type3\"]\n\nif pp!=1:\n  for sen in XX.index:\n      # Removiendo caracteres especiales\n      document = re.sub(r'\\W', ' ', str(XX[sen]))\n      \n      # Removiendo caracteres solitarios despues del inicio\n      document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n      \n      # Removiendo caracteres solitarios al inicio\n      document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n\n      # Removiendo el prefijo b\n      document = re.sub(r'^b\\s+', '', document) #solo cuando se agrega al inicio\n      \n      # Minúsculas\n      document = document.lower()\n\n      # Removiendo caracteres solitarios al inicio\n      document = re.sub('rt', '', document)\n\n      # Removiendo caracteres solitarios al inicio\n      document = re.sub('http', '', document)\n\n      # Removiendo caracteres solitarios al inicio\n      document = re.sub('co', '', document)\n\n      # Removiendo caracteres solitarios al inicio\n      document = re.sub('_', '', document) \n\n      # Removiendo multiples espacios\n      document = re.sub(r'\\s+', ' ', document, flags=re.I)\n      \n      # Lematizacion\n      document = nlp (document)\n      lemmas = [no_stop.lemma_ for no_stop in document if not no_stop.is_stop]\n      document = ' '.join(lemmas)\n\n      # Removiendo caracteres solitarios al inicio\n      document = re.sub('él', '', document)\n\n      # Removiendo caracteres solitarios despues del inicio\n      document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n      \n      # Removiendo caracteres solitarios al inicio\n      document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n  \n      documents.append(document)\n      j = j+1\n      print(j)\n\n  # Longitud\n  text_len = []\n  for text in documents:\n      tweet_len = len(text.split())\n      text_len.append(tweet_len)\n\n  dff[\"tweet_text_translated_clean\"]=documents\n  dff['text_translated_clean_len'] = text_len\n  dff=dff.loc[~pd.isnull(dff[\"tweet_text_translated_clean\"])]\n\nplt.figure(figsize=(7,5))\nax = sns.countplot(x='text_translated_clean_len', data=dff, palette='magma')\nplt.title('Conteo de textos limpiados por número de palabras (español)', fontsize=20)\nplt.yticks([])\nplt.xticks([])\n#for i in ax.containers:\n#    ax.bar_label(i,)\nplt.ylabel('conteo')\nplt.xlabel('')\nplt.show()\nprint(dff['text_translated_clean_len'].describe())\n\nfrom collections import Counter\nresults = set()\ndff['tweet_text_translated_clean'].str.lower().str.split().apply(results.update)\nresults = Counter()\ndff['tweet_text_translated_clean'].str.lower().str.split().apply(results.update)\nprint(len(results))\n#print(results.most_common())\n\nyy = [count for tag, count in results.most_common(20)]\nx = [tag for tag, count in results.most_common(20)]\n\nplt.barh(x,yy, color='crimson')\n\n# Vectorizar\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(max_features=1000, min_df=5, max_df=0.7)\nXX = vectorizer.fit_transform(dff['tweet_text_translated_clean']).toarray()\n\n# TF-IDF\nfrom sklearn.feature_extraction.text import TfidfTransformer\ntfidfconverter = TfidfTransformer()\nXX = tfidfconverter.fit_transform(XX).toarray()\n\n# Separando (equivalente al de arriba por el random state)\nX_train, X_test, y_train, y_test = train_test_split(XX, y, test_size=0.2,random_state=17)\nprint(\"Hay por procesar:\", len(X_test))","metadata":{"id":"MdaQgmTEpadm","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8bd4e325-fd3d-494e-d4fd-1e7145191743","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## + RF","metadata":{"id":"OQ3EuPZJZKES"}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=1000, random_state=0)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\ncooler_confusion_matrix(y_test,y_pred,\"RF\",etiquetas)\nprint(classification_report(y_test,y_pred, target_names=etiquetas))\n\n# Escribe tu propia reseña y prueba tu modelo\nreview1 = \"Me gusta las finanzas\"\nreview1_features = get_features(review1)\nprint(\"Review:\", review1)\nprint(\"Probabilidad de no bullying:\", rf.predict_proba(review1_features)[0,0]) #0 es por la primera obs, y 0 es por prob de no bullying\nprint(\"Probabilidad de bullying por edad:\", rf.predict_proba(review1_features)[0,1]) #0 es por la primera obs, y 0 es por prob de no bullying\nprint(\"Probabilidad de bullying por género:\", rf.predict_proba(review1_features)[0,2]) #0 es por la primera obs, y 0 es por prob de no bullying\nprint(\"Probabilidad de bullying por etnicidad:\", rf.predict_proba(review1_features)[0,3]) #0 es por la primera obs, y 0 es por prob de no bullying\nprint(\"Probabilidad de bullying por religión:\", rf.predict_proba(review1_features)[0,4]) #0 es por la primera obs, y 0 es por prob de no bullying\nprint(\"Probabilidad de bullying por otro tipo:\", rf.predict_proba(review1_features)[0,5]) #0 es por la primera obs, y 0 es por prob de no bullying","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":862},"id":"_KHs6BgRZKVl","outputId":"5889f949-d236-490e-bc52-aa2b8b78c4f5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## + SVM","metadata":{"id":"5CbCRVys_o4c"}},{"cell_type":"code","source":"from sklearn.svm import SVC\nsv = SVC(random_state=17)\nsv.fit(X_train, y_train)\ny_pred = sv.predict(X_test)\ncooler_confusion_matrix(y_test,y_pred,\"SVM\",etiquetas)\nprint(classification_report(y_test,y_pred, target_names=etiquetas))\n\n# Escribe tu propia reseña y prueba tu modelo\nreview1 = \"Perra loca\"\nreview1_features = get_features(review1)\nprint(\"Review:\", review1)\nprint(\"Probabilidad de no bullying:\", sv.predict(review1_features.toarray())) #0 es por la primera obs, y 0 es por prob de no bullying","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":775},"id":"uZwJXMPt_pRs","outputId":"f0c152c1-110e-45f2-b495-d5d07d3ba135"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## + Naive Bayes","metadata":{"id":"xGpAp-Ua__m7"}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\nnb.fit(X_train, y_train)\ny_pred = nb.predict(X_test)\ncooler_confusion_matrix(y_test,y_pred,\"NB\",etiquetas)\nprint(classification_report(y_test,y_pred, target_names=etiquetas))\n\n# Escribe tu propia reseña y prueba tu modelo\nreview1 = \"Negra, a que hora vas por el pan?\"\nreview1_features = get_features(review1)\nprint(\"Review:\", review1)\nprint(\"Probabilidad de no bullying:\", nb.predict_proba(review1_features)[0,0]) #0 es por la primera obs, y 0 es por prob de no bullying\nprint(\"Probabilidad de bullying por edad:\", nb.predict_proba(review1_features)[0,1]) #0 es por la primera obs, y 0 es por prob de no bullying\nprint(\"Probabilidad de bullying por género:\", nb.predict_proba(review1_features)[0,2]) #0 es por la primera obs, y 0 es por prob de no bullying\nprint(\"Probabilidad de bullying por etnicidad:\", nb.predict_proba(review1_features)[0,3]) #0 es por la primera obs, y 0 es por prob de no bullying\nprint(\"Probabilidad de bullying por religión:\", nb.predict_proba(review1_features)[0,4]) #0 es por la primera obs, y 0 es por prob de no bullying\nprint(\"Probabilidad de bullying por otro tipo:\", nb.predict_proba(review1_features)[0,5]) #0 es por la primera obs, y 0 es por prob de no bullying","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":862},"id":"PLk_waPv__5R","outputId":"1d091ba1-14b8-4183-fcdc-a1200e965a7f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## + Redes Neuronales","metadata":{"id":"tsAA07MgAuMB"}},{"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nnn = MLPClassifier(hidden_layer_sizes=(150,100,50,10), max_iter=300, activation = 'relu',solver='sgd',random_state=17)\nnn.fit(X_train, y_train)\ny_pred = nn.predict(X_test)\ncooler_confusion_matrix(y_test,y_pred,\"Redes Neuronales\",etiquetas)\nprint(classification_report(y_test,y_pred, target_names=etiquetas))\n\n# Escribe tu propia reseña y prueba tu modelo\nreview1 = \"Perra loca\"\nreview1_features = get_features(review1)\nprint(\"Review:\", review1)\nprint(\"Probabilidad de no bullying:\", nn.predict_proba(review1_features)[0,0]) #0 es por la primera obs, y 0 es por prob de no bullying\nprint(\"Probabilidad de bullying por edad:\", nn.predict_proba(review1_features)[0,1]) #0 es por la primera obs, y 0 es por prob de no bullying\nprint(\"Probabilidad de bullying por género:\", nn.predict_proba(review1_features)[0,2]) #0 es por la primera obs, y 0 es por prob de no bullying\nprint(\"Probabilidad de bullying por etnicidad:\", nn.predict_proba(review1_features)[0,3]) #0 es por la primera obs, y 0 es por prob de no bullying\nprint(\"Probabilidad de bullying por religión:\", nn.predict_proba(review1_features)[0,4]) #0 es por la primera obs, y 0 es por prob de no bullying\nprint(\"Probabilidad de bullying por otro tipo:\", nn.predict_proba(review1_features)[0,5]) #0 es por la primera obs, y 0 es por prob de no bullying","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"6v4jUA6YAugC","outputId":"c3a6d2c5-589d-43f7-bcee-e526f36a6fe8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Calibración","metadata":{"id":"Qad2eVdSTpnT"}},{"cell_type":"code","source":"plt.clf()\n\n# Calibración\nprint(rf.get_params())\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {'max_depth': [3,5,7,10], 'n_estimators': [500,1000,1500], 'max_features': ['auto','sqrt']}\n\nmax_depth=[50,70,100,None]\nn_estimators=[500,1000,1500]\nmax_features=['auto','sqrt']\n\nfor i in max_depth:\n  for j in n_estimators:\n    for k in max_features:\n      print(\"Probando con max_depth de\", str(i), \", un n_estimators de\", str(j),\" y un max_features\", k)\n      model=RandomForestClassifier(max_depth=i, n_estimators=j, max_features=k, random_state=0).fit(X_train,y_train)\n      y_pred = model.predict(X_test)\n      cooler_confusion_matrix(y_test,y_pred,\"Modelo\",etiquetas)\n      print(classification_report(y_test,y_pred, target_names=etiquetas))\n\n\nmodel=RandomForestClassifier(max_depth=None, n_estimators=1000, max_features='auto', random_state=0).fit(X_train,y_train)\ny_pred = model.predict(X_test)\ncooler_confusion_matrix(y_test,y_pred,\"Modelo\",etiquetas)\nprint(classification_report(y_test,y_pred, target_names=etiquetas))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"bQAO9IMjTqhn","outputId":"70c90297-6bbd-41fb-ba51-2fd585b43f54"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## BETO","metadata":{"id":"Raq9K5zJcTcv"}},{"cell_type":"code","source":"# Tutorial: https://www.youtube.com/watch?v=wp9BudYGZyA&t=1862s\n### Preparación general\n# Librerías para BETO\nfrom tqdm.auto import tqdm\nimport tensorflow as tf\nfrom transformers import BertTokenizer\nmodelo='dccuchile/bert-base-spanish-wwm-uncased'\nvariable_x=\"tweet_text_translated\"\nnombre_modelo='bullying_beto_modelo'\nz=16 # Batch size\nepocas=3 # N° épocas\n\nX = dff[[\"Unnamed: 0\",'tweet_text','tweet_text_translated','prediccion_jonatan']]\n# train-test-split para documents (textos sin íconos especiales ni doble espaciado)\nX_train, X_test, y_train, y_test = train_test_split(X[variable_x], y, test_size=0.2,random_state=17)\nprint(\"Hay por procesar:\", len(X_test))\nprint(X_test)\n\n# Trayendo al modelo de Hugging Face\ntokenizer = BertTokenizer.from_pretrained(modelo)\n\n# Tokenizando\ntoken = tokenizer.encode_plus(\n    X_train.iloc[0],\n    max_length=256, \n    truncation=True, \n    padding='max_length', \n    add_special_tokens=True,\n    return_tensors='tf'\n)\nprint(token.input_ids)\nX_input_ids = np.zeros((len(X_train), 256))\nX_attn_masks = np.zeros((len(X_train), 256))\n\n# Funciones\ndef generate_training_data(df, ids, masks, tokenizer):\n    for i, text in tqdm(enumerate(X_train)):\n        tokenized_text = tokenizer.encode_plus(\n            text,\n            max_length=256, \n            truncation=True, \n            padding='max_length', \n            add_special_tokens=True,\n            return_tensors='tf'\n        )\n        ids[i, :] = tokenized_text.input_ids\n        masks[i, :] = tokenized_text.attention_mask\n    return ids, masks\n\n# Creando efectivamente la tokenización\nX_input_ids, X_attn_masks = generate_training_data(X_train, X_input_ids, X_attn_masks, tokenizer)\nlabels = np.zeros((len(X_train), 6))\nlabels[np.arange(len(X_train)), y_train] = 1\n\n# Pasando a tensorflow form\ndataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))\n#dataset.take(2) # one sample data\n\ndef SentimentDatasetMapFunction(input_ids, attn_masks, labels):\n    return {\n        'input_ids': input_ids,\n        'attention_mask': attn_masks\n    }, labels\n\n### Preparación modelo\n# Seteando modelo\nprint(\"Batch size de:\",z,\"\\nNúmero de épocas de:\",epocas)\ndataset = dataset.map(SentimentDatasetMapFunction)\nprint(len(dataset))\ndataset = dataset.batch(z, drop_remainder=True)\nprint(len(dataset))\np = 0.8 # Porcentaje de datos para entrenar\ntrain_size = int((len(X_train)//z)*p)\ntrain_dataset = dataset.take(train_size)\nprint(len(train_dataset))\nval_dataset = dataset.skip(train_size)\nprint(len(val_dataset))\n\nfrom transformers import TFBertModel\nmodel = TFBertModel.from_pretrained(modelo)\n\n# Hay 2 inputs, los id y los attention mask\ninput_ids = tf.keras.layers.Input(shape=(256,), name='input_ids', dtype='int32')\nattn_masks = tf.keras.layers.Input(shape=(256,), name='attention_mask', dtype='int32')\n\n# Los embeddings\nbert_embds = model.bert(input_ids, attention_mask=attn_masks)[1] # 0 -> activation layer (3D), 1 -> pooled output layer (2D)\nintermediate_layer = tf.keras.layers.Dense(512, activation='relu', name='intermediate_layer')(bert_embds)\noutput_layer = tf.keras.layers.Dense(6, activation='softmax', name='output_layer')(intermediate_layer) # softmax -> calcs probs of classes\n\n# Arquitectura\nsentiment_model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)\nsentiment_model.summary()\n\n# Estimando modelo\noptim = tf.keras.optimizers.Adam(learning_rate=1e-5, decay=1e-6)\nloss_func = tf.keras.losses.CategoricalCrossentropy()\naccc = tf.keras.metrics.CategoricalAccuracy('accuracy')\nsentiment_model.compile(optimizer=optim, loss=loss_func, metrics=[accc])\nhist = sentiment_model.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=epocas\n)\n\n# Guardando\nsentiment_model.save(nombre_modelo)\n\n# Cargando\nsentiment_model = tf.keras.models.load_model(nombre_modelo)\ntokenizer = BertTokenizer.from_pretrained(modelo)\n\n### Evaluando lo obtenido\n# Funciones\ndef prepare_data(input_text, tokenizer):\n    token = tokenizer.encode_plus(\n        input_text,\n        max_length=256, \n        truncation=True, \n        padding='max_length', \n        add_special_tokens=True,\n        return_tensors='tf'\n    )\n    return {\n        'input_ids': tf.cast(token.input_ids, tf.float64),\n        'attention_mask': tf.cast(token.attention_mask, tf.float64)\n    }\n\ndef make_prediction(model, processed_data, classes=etiquetas):\n    probs = model.predict(processed_data)[0]\n    return classes[np.argmax(probs)]\n\n# Evaluando general\npd.DataFrame(hist.history).plot(figsize=(8,5))\nplt.grid(True)\nplt.gca().set_ylim(0,1)\nplt.show()\n\n# Evaluando específico\ninput_text = 'Parecía carbón'\nprocessed_data = prepare_data(input_text, tokenizer)\nresult = make_prediction(sentiment_model, processed_data=processed_data)\nprint(f\"Predicted Sentiment: {result}\")\n#!zip -r /content/file.zip /content/bullying_beto_modelo\n\ny_pred = []\np=1\nfor i in X_test:\n  #print(p)\n  processed_data = prepare_data(i, tokenizer)\n  result = make_prediction(sentiment_model, processed_data=processed_data)\n  #print(f\"Predicted Sentiment: {result}\")\n  if result==\"No\":\n    y_pred.append(0)\n  elif result==\"Edad\":\n    y_pred.append(1)\n  elif result==\"Género\":\n    y_pred.append(2)\n  elif result==\"Etnicidad\":\n    y_pred.append(3)\n  elif result==\"Religión\":\n    y_pred.append(4)\n  elif result==\"Otro tipo\":\n    y_pred.append(5)\n  p+=1\n\n# Para Excel\ncooler_confusion_matrix(y_test,y_pred,\"BETO normal\",etiquetas)\nprint(classification_report(y_test,y_pred, target_names=etiquetas))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M1h-lECLcT-H","outputId":"c35a4494-b09e-4e16-dabb-5ae69270937c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tutorial: https://www.youtube.com/watch?v=wp9BudYGZyA&t=1862s\n### Preparación general\n# Librerías para BETO\nfrom tqdm.auto import tqdm\nimport tensorflow as tf\nfrom transformers import BertTokenizer\nmodelo='dccuchile/bert-base-spanish-wwm-uncased'\nvariable_x=\"tweet_text_translated_clean\"\nnombre_modelo='bullying_beto_modelo'\nz=16 # Batch size\nepocas=3 # N° épocas\n\nX = dff[[\"Unnamed: 0\",'tweet_text','tweet_text_translated','tweet_text_translated_clean','prediccion_jonatan']]\n# train-test-split para documents (textos sin íconos especiales ni doble espaciado)\nX_train, X_test, y_train, y_test = train_test_split(X[variable_x], y, test_size=0.2,random_state=17)\nprint(\"Hay por procesar:\", len(X_test))\nprint(X_test)\n\n# Trayendo al modelo de Hugging Face\ntokenizer = BertTokenizer.from_pretrained(modelo)\n\n# Tokenizando\ntoken = tokenizer.encode_plus(\n    X_train.iloc[0],\n    max_length=256, \n    truncation=True, \n    padding='max_length', \n    add_special_tokens=True,\n    return_tensors='tf'\n)\nprint(token.input_ids)\nX_input_ids = np.zeros((len(X_train), 256))\nX_attn_masks = np.zeros((len(X_train), 256))\n\n# Funciones\ndef generate_training_data(df, ids, masks, tokenizer):\n    for i, text in tqdm(enumerate(X_train)):\n        tokenized_text = tokenizer.encode_plus(\n            text,\n            max_length=256, \n            truncation=True, \n            padding='max_length', \n            add_special_tokens=True,\n            return_tensors='tf'\n        )\n        ids[i, :] = tokenized_text.input_ids\n        masks[i, :] = tokenized_text.attention_mask\n    return ids, masks\n\n# Creando efectivamente la tokenización\nX_input_ids, X_attn_masks = generate_training_data(X_train, X_input_ids, X_attn_masks, tokenizer)\nlabels = np.zeros((len(X_train), 6))\nlabels[np.arange(len(X_train)), y_train] = 1\n\n# Pasando a tensorflow form\ndataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))\n#dataset.take(2) # one sample data\n\ndef SentimentDatasetMapFunction(input_ids, attn_masks, labels):\n    return {\n        'input_ids': input_ids,\n        'attention_mask': attn_masks\n    }, labels\n\n### Preparación modelo\n# Seteando modelo\nprint(\"Batch size de:\",z,\"\\nNúmero de épocas de:\",epocas)\ndataset = dataset.map(SentimentDatasetMapFunction)\nprint(len(dataset))\ndataset = dataset.batch(z, drop_remainder=True)\nprint(len(dataset))\np = 0.8 # Porcentaje de datos para entrenar\ntrain_size = int((len(X_train)//z)*p)\ntrain_dataset = dataset.take(train_size)\nprint(len(train_dataset))\nval_dataset = dataset.skip(train_size)\nprint(len(val_dataset))\n\nfrom transformers import TFBertModel\nmodel = TFBertModel.from_pretrained(modelo)\n\n# Hay 2 inputs, los id y los attention mask\ninput_ids = tf.keras.layers.Input(shape=(256,), name='input_ids', dtype='int32')\nattn_masks = tf.keras.layers.Input(shape=(256,), name='attention_mask', dtype='int32')\n\n# Los embeddings\nbert_embds = model.bert(input_ids, attention_mask=attn_masks)[1] # 0 -> activation layer (3D), 1 -> pooled output layer (2D)\nintermediate_layer = tf.keras.layers.Dense(512, activation='relu', name='intermediate_layer')(bert_embds)\noutput_layer = tf.keras.layers.Dense(6, activation='softmax', name='output_layer')(intermediate_layer) # softmax -> calcs probs of classes\n\n# Arquitectura\nsentiment_model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)\nsentiment_model.summary()\n\n# Estimando modelo\noptim = tf.keras.optimizers.Adam(learning_rate=1e-5, decay=1e-6)\nloss_func = tf.keras.losses.CategoricalCrossentropy()\naccc = tf.keras.metrics.CategoricalAccuracy('accuracy')\nsentiment_model.compile(optimizer=optim, loss=loss_func, metrics=[accc])\nhist = sentiment_model.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=epocas\n)\n\n# Guardando\nsentiment_model.save(nombre_modelo)\n\n# Cargando\nsentiment_model = tf.keras.models.load_model(nombre_modelo)\ntokenizer = BertTokenizer.from_pretrained(modelo)\n\n### Evaluando lo obtenido\n# Funciones\ndef prepare_data(input_text, tokenizer):\n    token = tokenizer.encode_plus(\n        input_text,\n        max_length=256, \n        truncation=True, \n        padding='max_length', \n        add_special_tokens=True,\n        return_tensors='tf'\n    )\n    return {\n        'input_ids': tf.cast(token.input_ids, tf.float64),\n        'attention_mask': tf.cast(token.attention_mask, tf.float64)\n    }\n\ndef make_prediction(model, processed_data, classes=etiquetas):\n    probs = model.predict(processed_data)[0]\n    return classes[np.argmax(probs)]\n\n# Evaluando general\npd.DataFrame(hist.history).plot(figsize=(8,5))\nplt.grid(True)\nplt.gca().set_ylim(0,1)\nplt.show()\n\n# Evaluando específico\ninput_text = 'Parecía carbón'\nprocessed_data = prepare_data(input_text, tokenizer)\nresult = make_prediction(sentiment_model, processed_data=processed_data)\nprint(f\"Predicted Sentiment: {result}\")\n#!zip -r /content/file.zip /content/bullying_beto_modelo\n\ny_pred = []\np=1\nfor i in X_test:\n  #print(p)\n  processed_data = prepare_data(i, tokenizer)\n  result = make_prediction(sentiment_model, processed_data=processed_data)\n  #print(f\"Predicted Sentiment: {result}\")\n  if result==\"No\":\n    y_pred.append(0)\n  elif result==\"Edad\":\n    y_pred.append(1)\n  elif result==\"Género\":\n    y_pred.append(2)\n  elif result==\"Etnicidad\":\n    y_pred.append(3)\n  elif result==\"Religión\":\n    y_pred.append(4)\n  elif result==\"Otro tipo\":\n    y_pred.append(5)\n  p+=1\n\n# Para Excel\ncooler_confusion_matrix(y_test,y_pred,\"BETO clean\",etiquetas)\nprint(classification_report(y_test,y_pred, target_names=etiquetas))","metadata":{"id":"52Ret2Tcd3jc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inglés","metadata":{"id":"McYXL9emHeHh"}},{"cell_type":"markdown","source":"## Estadística descriptiva","metadata":{"id":"R8wmDEiFHtQF"}},{"cell_type":"code","source":"# Modelando\nf1=[]\nacc=[]\nprec=[]\nspec=[]\nsens=[]\nvpp=[]\nvpn=[]\nauc=[]\nmod=[]\ndat=[]\n\netiquetas=[\"No\",\"Edad\",\"Género\",\"Etnicidad\",\"Religión\",\"Otro tipo\"]","metadata":{"id":"Eyh31H3CHvDU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TF - IDF","metadata":{"id":"QmXVs2ZKH4PC"}},{"cell_type":"code","source":"nlp = en_core_web_md.load()\ndocuments = []\nj = 0\nX = dff[[\"Unnamed: 0\",'tweet_text','tweet_text_translated','prediccion_jonatan']]\nXX = X[\"tweet_text\"]\ny = yyy[\"cyberbullying_type3\"]\n\nif pp!=1:\n  for sen in XX.index:\n      # Removiendo caracteres especiales\n      document = re.sub(r'\\W', ' ', str(XX[sen]))\n      \n      # Removiendo caracteres solitarios despues del inicio\n      document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n      \n      # Removiendo caracteres solitarios al inicio\n      document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n\n      # Removiendo el prefijo b\n      document = re.sub(r'^b\\s+', '', document) #solo cuando se agrega al inicio\n      \n      # Minúsculas\n      document = document.lower()\n\n      # Removiendo caracteres solitarios al inicio\n      document = re.sub('rt', '', document)\n\n      # Removiendo caracteres solitarios al inicio\n      document = re.sub('http', '', document)\n\n      # Removiendo caracteres solitarios al inicio\n      document = re.sub('co', '', document)\n\n      # Removiendo caracteres solitarios al inicio\n      document = re.sub('_', '', document) \n\n      # Removiendo multiples espacios\n      document = re.sub(r'\\s+', ' ', document, flags=re.I)\n      \n      # Lematizacion\n      document = nlp (document)\n      lemmas = [no_stop.lemma_ for no_stop in document if not no_stop.is_stop]\n      document = ' '.join(lemmas)\n\n      # Removiendo caracteres solitarios al inicio\n      document = re.sub('él', '', document)\n\n      # Removiendo caracteres solitarios despues del inicio\n      document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n      \n      # Removiendo caracteres solitarios al inicio\n      document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n  \n      documents.append(document)\n      j = j+1\n      print(j)\n\n  # Longitud\n  text_len = []\n  for text in documents:\n      tweet_len = len(text.split())\n      text_len.append(tweet_len)\n\n  dff[\"tweet_text_translated_clean\"]=documents\n  dff['text_translated_clean_len'] = text_len\n  dff=dff.loc[~pd.isnull(dff[\"tweet_text_translated_clean\"])]\n\n# Vectorizar\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(max_features=1000, min_df=5, max_df=0.7)\nXX = vectorizer.fit_transform(dff['tweet_text_clean']).toarray()\n\n# TF-IDF\nfrom sklearn.feature_extraction.text import TfidfTransformer\ntfidfconverter = TfidfTransformer()\nXX = tfidfconverter.fit_transform(XX).toarray()\n\n# Separando (equivalente al de arriba por el random state)\nX_train, X_test, y_train, y_test = train_test_split(XX, y, test_size=0.2,random_state=17)\nprint(\"Hay por procesar:\", len(X_test))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0N_hapaIH6R0","outputId":"19677dcc-f258-469b-e3c0-0f5ddfceac3a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## + RF","metadata":{"id":"YKl1x1LnJgMj"}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=1000, random_state=0)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\ncooler_confusion_matrix(y_test,y_pred,\"RF\",etiquetas)\nprint(classification_report(y_test,y_pred, target_names=etiquetas))\n\n# Escribe tu propia reseña y prueba tu modelo\nreview1 = \"Me gusta las finanzas\"\nreview1_features = get_features(review1)\nprint(\"Review:\", review1)\nprint(\"Probabilidad de no bullying:\", rf.predict_proba(review1_features)[0,0]) #0 es por la primera obs, y 0 es por prob de no bullying\nprint(\"Probabilidad de bullying por edad:\", rf.predict_proba(review1_features)[0,1]) #0 es por la primera obs, y 0 es por prob de no bullying\nprint(\"Probabilidad de bullying por género:\", rf.predict_proba(review1_features)[0,2]) #0 es por la primera obs, y 0 es por prob de no bullying\nprint(\"Probabilidad de bullying por etnicidad:\", rf.predict_proba(review1_features)[0,3]) #0 es por la primera obs, y 0 es por prob de no bullying\nprint(\"Probabilidad de bullying por religión:\", rf.predict_proba(review1_features)[0,4]) #0 es por la primera obs, y 0 es por prob de no bullying\nprint(\"Probabilidad de bullying por otro tipo:\", rf.predict_proba(review1_features)[0,5]) #0 es por la primera obs, y 0 es por prob de no bullying","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":862},"id":"Xo5DYS1UJk7r","outputId":"fe3a866f-3a05-4bb9-e328-f5eda39990e1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## + SVM","metadata":{"id":"dT22ofjsJvPi"}},{"cell_type":"code","source":"from sklearn.svm import SVC\nsv = SVC(random_state=17)\nsv.fit(X_train, y_train)\ny_pred = sv.predict(X_test)\ncooler_confusion_matrix(y_test,y_pred,\"SVM\",etiquetas)\nprint(classification_report(y_test,y_pred, target_names=etiquetas))\n\n# Escribe tu propia reseña y prueba tu modelo\nreview1 = \"Fuck you bitch\"\nreview1_features = get_features(review1)\nprint(\"Review:\", review1)\nprint(\"Probabilidad de no bullying:\", sv.predict(review1_features.toarray())) #0 es por la primera obs, y 0 es por prob de no bullying","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":775},"id":"kTpdsGzwJvk0","outputId":"fdd1cc0d-b27f-4657-b80e-e88f33ab0663"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## + Naive Bayes","metadata":{"id":"wO2gAFXgJ9kE"}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\nnb.fit(X_train, y_train)\ny_pred = nb.predict(X_test)\ncooler_confusion_matrix(y_test,y_pred,\"NB\",etiquetas)\nprint(classification_report(y_test,y_pred, target_names=etiquetas))\n\n# Escribe tu propia reseña y prueba tu modelo\nreview1 = \"Negra, a que hora vas por el pan?\"\nreview1_features = get_features(review1)\nprint(\"Review:\", review1)\nprint(\"Probabilidad de no bullying:\", nb.predict_proba(review1_features)[0,0]) #0 es por la primera obs, y 0 es por prob de no bullying\nprint(\"Probabilidad de bullying por edad:\", nb.predict_proba(review1_features)[0,1]) #0 es por la primera obs, y 0 es por prob de no bullying\nprint(\"Probabilidad de bullying por género:\", nb.predict_proba(review1_features)[0,2]) #0 es por la primera obs, y 0 es por prob de no bullying\nprint(\"Probabilidad de bullying por etnicidad:\", nb.predict_proba(review1_features)[0,3]) #0 es por la primera obs, y 0 es por prob de no bullying\nprint(\"Probabilidad de bullying por religión:\", nb.predict_proba(review1_features)[0,4]) #0 es por la primera obs, y 0 es por prob de no bullying\nprint(\"Probabilidad de bullying por otro tipo:\", nb.predict_proba(review1_features)[0,5]) #0 es por la primera obs, y 0 es por prob de no bullying","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":862},"id":"fS8NjdhbJ-Am","outputId":"f6172f61-1692-4e1d-f17f-f899c7753e18"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## + Redes Neuronales","metadata":{"id":"T10r9Wr_KFHb"}},{"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nnn = MLPClassifier(hidden_layer_sizes=(150,100,50,10), max_iter=300, activation = 'relu',solver='sgd',random_state=17)\nnn.fit(X_train, y_train)\ny_pred = nn.predict(X_test)\ncooler_confusion_matrix(y_test,y_pred,\"Redes Neuronales\",etiquetas)\nprint(classification_report(y_test,y_pred, target_names=etiquetas))\n\n# Escribe tu propia reseña y prueba tu modelo\nreview1 = \"Perra loca\"\nreview1_features = get_features(review1)\nprint(\"Review:\", review1)\nprint(\"Probabilidad de no bullying:\", nn.predict_proba(review1_features)[0,0]) #0 es por la primera obs, y 0 es por prob de no bullying\nprint(\"Probabilidad de bullying por edad:\", nn.predict_proba(review1_features)[0,1]) #0 es por la primera obs, y 0 es por prob de no bullying\nprint(\"Probabilidad de bullying por género:\", nn.predict_proba(review1_features)[0,2]) #0 es por la primera obs, y 0 es por prob de no bullying\nprint(\"Probabilidad de bullying por etnicidad:\", nn.predict_proba(review1_features)[0,3]) #0 es por la primera obs, y 0 es por prob de no bullying\nprint(\"Probabilidad de bullying por religión:\", nn.predict_proba(review1_features)[0,4]) #0 es por la primera obs, y 0 es por prob de no bullying\nprint(\"Probabilidad de bullying por otro tipo:\", nn.predict_proba(review1_features)[0,5]) #0 es por la primera obs, y 0 es por prob de no bullying","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":917},"id":"aCJLDZG3KFew","outputId":"483b886d-a498-48ee-b717-bb97ff806a37"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Calibración","metadata":{"id":"4_nRKh7sKOCR"}},{"cell_type":"code","source":"plt.clf()\n\n# Calibración\nprint(rf.get_params())\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {'max_depth': [3,5,7,10], 'n_estimators': [500,1000,1500], 'max_features': ['auto','sqrt']}\n\nmax_depth=[50,70,100,None]\nn_estimators=[500,1000,1500]\nmax_features=['auto','sqrt']\n\nfor i in max_depth:\n  for j in n_estimators:\n    for k in max_features:\n      print(\"Probando con max_depth de\", str(i), \", un n_estimators de\", str(j),\" y un max_features\", k)\n      model=RandomForestClassifier(max_depth=i, n_estimators=j, max_features=k, random_state=0).fit(X_train,y_train)\n      y_pred = model.predict(X_test)\n      cooler_confusion_matrix(y_test,y_pred,\"Modelo\",etiquetas)\n      print(classification_report(y_test,y_pred, target_names=etiquetas))\n\n\nmodel=RandomForestClassifier(max_depth=None, n_estimators=1000, max_features='auto', random_state=0).fit(X_train,y_train)\ny_pred = model.predict(X_test)\ncooler_confusion_matrix(y_test,y_pred,\"Modelo\",etiquetas)\nprint(classification_report(y_test,y_pred, target_names=etiquetas))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"XRsuo9R3KUhZ","outputId":"0db28cb4-0143-42a7-fb00-4d5b8ec9da8e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## BERT","metadata":{"id":"LIkTswfQ7kb_"}},{"cell_type":"code","source":"# Tutorial: https://www.youtube.com/watch?v=wp9BudYGZyA&t=1862s\n### Preparación general\n# Librerías para BETO\nfrom tqdm.auto import tqdm\nimport tensorflow as tf\nfrom transformers import BertTokenizer\nmodelo='bert-base-cased'\nvariable_x=\"tweet_text\"\nnombre_modelo='bullying_bert_model'\nz=16 # Batch size\nepocas=3 # N° épocas\n\nX = dff[[\"Unnamed: 0\",'tweet_text','tweet_text_translated','prediccion_jonatan','tweet_text_clean']]\n# train-test-split para documents (textos sin íconos especiales ni doble espaciado)\nX_train, X_test, y_train, y_test = train_test_split(X[variable_x], y, test_size=0.2,random_state=17)\nprint(\"Hay por procesar:\", len(X_test))\nprint(X_test)\n\n# Trayendo al modelo de Hugging Face\ntokenizer = BertTokenizer.from_pretrained(modelo)\n\n# Tokenizando\ntoken = tokenizer.encode_plus(\n    X_train.iloc[0],\n    max_length=256, \n    truncation=True, \n    padding='max_length', \n    add_special_tokens=True,\n    return_tensors='tf'\n)\nprint(token.input_ids)\nX_input_ids = np.zeros((len(X_train), 256))\nX_attn_masks = np.zeros((len(X_train), 256))\n\n# Funciones\ndef generate_training_data(df, ids, masks, tokenizer):\n    for i, text in tqdm(enumerate(X_train)):\n        tokenized_text = tokenizer.encode_plus(\n            text,\n            max_length=256, \n            truncation=True, \n            padding='max_length', \n            add_special_tokens=True,\n            return_tensors='tf'\n        )\n        ids[i, :] = tokenized_text.input_ids\n        masks[i, :] = tokenized_text.attention_mask\n    return ids, masks\n\n# Creando efectivamente la tokenización\nX_input_ids, X_attn_masks = generate_training_data(X_train, X_input_ids, X_attn_masks, tokenizer)\nlabels = np.zeros((len(X_train), 6))\nlabels[np.arange(len(X_train)), y_train] = 1\n\n# Pasando a tensorflow form\ndataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))\n#dataset.take(2) # one sample data\n\ndef SentimentDatasetMapFunction(input_ids, attn_masks, labels):\n    return {\n        'input_ids': input_ids,\n        'attention_mask': attn_masks\n    }, labels\n\n### Preparación modelo\n# Seteando modelo\nprint(\"Batch size de:\",z,\"\\nNúmero de épocas de:\",epocas)\ndataset = dataset.map(SentimentDatasetMapFunction)\nprint(len(dataset))\ndataset = dataset.batch(z, drop_remainder=True)\nprint(len(dataset))\np = 0.8 # Porcentaje de datos para entrenar\ntrain_size = int((len(X_train)//z)*p)\ntrain_dataset = dataset.take(train_size)\nprint(len(train_dataset))\nval_dataset = dataset.skip(train_size)\nprint(len(val_dataset))\n\nfrom transformers import TFBertModel\nmodel = TFBertModel.from_pretrained(modelo)\n\n# Hay 2 inputs, los id y los attention mask\ninput_ids = tf.keras.layers.Input(shape=(256,), name='input_ids', dtype='int32')\nattn_masks = tf.keras.layers.Input(shape=(256,), name='attention_mask', dtype='int32')\n\n# Los embeddings\nbert_embds = model.bert(input_ids, attention_mask=attn_masks)[1] # 0 -> activation layer (3D), 1 -> pooled output layer (2D)\nintermediate_layer = tf.keras.layers.Dense(512, activation='relu', name='intermediate_layer')(bert_embds)\noutput_layer = tf.keras.layers.Dense(6, activation='softmax', name='output_layer')(intermediate_layer) # softmax -> calcs probs of classes\n\n# Arquitectura\nsentiment_model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)\nsentiment_model.summary()\n\n# Estimando modelo\noptim = tf.keras.optimizers.Adam(learning_rate=1e-5, decay=1e-6)\nloss_func = tf.keras.losses.CategoricalCrossentropy()\naccc = tf.keras.metrics.CategoricalAccuracy('accuracy')\nsentiment_model.compile(optimizer=optim, loss=loss_func, metrics=[accc])\nhist = sentiment_model.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=epocas\n)\n\n# Guardando\nsentiment_model.save(nombre_modelo)\n\n# Cargando\nsentiment_model = tf.keras.models.load_model(nombre_modelo)\ntokenizer = BertTokenizer.from_pretrained(modelo)\n\n### Evaluando lo obtenido\n# Funciones\ndef prepare_data(input_text, tokenizer):\n    token = tokenizer.encode_plus(\n        input_text,\n        max_length=256, \n        truncation=True, \n        padding='max_length', \n        add_special_tokens=True,\n        return_tensors='tf'\n    )\n    return {\n        'input_ids': tf.cast(token.input_ids, tf.float64),\n        'attention_mask': tf.cast(token.attention_mask, tf.float64)\n    }\n\ndef make_prediction(model, processed_data, classes=etiquetas):\n    probs = model.predict(processed_data)[0]\n    return classes[np.argmax(probs)]\n\n# Evaluando general\npd.DataFrame(hist.history).plot(figsize=(8,5))\nplt.grid(True)\nplt.gca().set_ylim(0,1)\nplt.show()\n\n# Evaluando específico\ninput_text = 'What a nice day'\nprocessed_data = prepare_data(input_text, tokenizer)\nresult = make_prediction(sentiment_model, processed_data=processed_data)\nprint(f\"Predicted Sentiment: {result}\")\n#!zip -r /content/file.zip /content/bullying_beto_modelo\n\ny_pred = []\np=1\nfor i in X_test:\n  #print(p)\n  processed_data = prepare_data(i, tokenizer)\n  result = make_prediction(sentiment_model, processed_data=processed_data)\n  #print(f\"Predicted Sentiment: {result}\")\n  if result==\"No\":\n    y_pred.append(0)\n  elif result==\"Edad\":\n    y_pred.append(1)\n  elif result==\"Género\":\n    y_pred.append(2)\n  elif result==\"Etnicidad\":\n    y_pred.append(3)\n  elif result==\"Religión\":\n    y_pred.append(4)\n  elif result==\"Otro tipo\":\n    y_pred.append(5)\n  p+=1\n\n# Para Excel\ncooler_confusion_matrix(y_test,y_pred,\"BERT normal\",etiquetas)\nprint(classification_report(y_test,y_pred, target_names=etiquetas))","metadata":{"id":"65anjB-R8wVa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tutorial: https://www.youtube.com/watch?v=wp9BudYGZyA&t=1862s\n### Preparación general\n# Librerías para BETO\nfrom tqdm.auto import tqdm\nimport tensorflow as tf\nfrom transformers import BertTokenizer\nmodelo='bert-base-cased'\nvariable_x=\"tweet_text_clean\"\nnombre_modelo='bullying_bert_model'\nz=16 # Batch size\nepocas=3 # N° épocas\n\nX = dff[[\"Unnamed: 0\",'tweet_text','tweet_text_translated','prediccion_jonatan','tweet_text_clean']]\n# train-test-split para documents (textos sin íconos especiales ni doble espaciado)\nX_train, X_test, y_train, y_test = train_test_split(X[variable_x], y, test_size=0.2,random_state=17)\nprint(\"Hay por procesar:\", len(X_test))\nprint(X_test)\n\n# Trayendo al modelo de Hugging Face\ntokenizer = BertTokenizer.from_pretrained(modelo)\n\n# Tokenizando\ntoken = tokenizer.encode_plus(\n    X_train.iloc[0],\n    max_length=256, \n    truncation=True, \n    padding='max_length', \n    add_special_tokens=True,\n    return_tensors='tf'\n)\nprint(token.input_ids)\nX_input_ids = np.zeros((len(X_train), 256))\nX_attn_masks = np.zeros((len(X_train), 256))\n\n# Funciones\ndef generate_training_data(df, ids, masks, tokenizer):\n    for i, text in tqdm(enumerate(X_train)):\n        tokenized_text = tokenizer.encode_plus(\n            text,\n            max_length=256, \n            truncation=True, \n            padding='max_length', \n            add_special_tokens=True,\n            return_tensors='tf'\n        )\n        ids[i, :] = tokenized_text.input_ids\n        masks[i, :] = tokenized_text.attention_mask\n    return ids, masks\n\n# Creando efectivamente la tokenización\nX_input_ids, X_attn_masks = generate_training_data(X_train, X_input_ids, X_attn_masks, tokenizer)\nlabels = np.zeros((len(X_train), 6))\nlabels[np.arange(len(X_train)), y_train] = 1\n\n# Pasando a tensorflow form\ndataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))\n#dataset.take(2) # one sample data\n\ndef SentimentDatasetMapFunction(input_ids, attn_masks, labels):\n    return {\n        'input_ids': input_ids,\n        'attention_mask': attn_masks\n    }, labels\n\n### Preparación modelo\n# Seteando modelo\nprint(\"Batch size de:\",z,\"\\nNúmero de épocas de:\",epocas)\ndataset = dataset.map(SentimentDatasetMapFunction)\nprint(len(dataset))\ndataset = dataset.batch(z, drop_remainder=True)\nprint(len(dataset))\np = 0.8 # Porcentaje de datos para entrenar\ntrain_size = int((len(X_train)//z)*p)\ntrain_dataset = dataset.take(train_size)\nprint(len(train_dataset))\nval_dataset = dataset.skip(train_size)\nprint(len(val_dataset))\n\nfrom transformers import TFBertModel\nmodel = TFBertModel.from_pretrained(modelo)\n\n# Hay 2 inputs, los id y los attention mask\ninput_ids = tf.keras.layers.Input(shape=(256,), name='input_ids', dtype='int32')\nattn_masks = tf.keras.layers.Input(shape=(256,), name='attention_mask', dtype='int32')\n\n# Los embeddings\nbert_embds = model.bert(input_ids, attention_mask=attn_masks)[1] # 0 -> activation layer (3D), 1 -> pooled output layer (2D)\nintermediate_layer = tf.keras.layers.Dense(512, activation='relu', name='intermediate_layer')(bert_embds)\noutput_layer = tf.keras.layers.Dense(6, activation='softmax', name='output_layer')(intermediate_layer) # softmax -> calcs probs of classes\n\n# Arquitectura\nsentiment_model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)\nsentiment_model.summary()\n\n# Estimando modelo\noptim = tf.keras.optimizers.Adam(learning_rate=1e-5, decay=1e-6)\nloss_func = tf.keras.losses.CategoricalCrossentropy()\naccc = tf.keras.metrics.CategoricalAccuracy('accuracy')\nsentiment_model.compile(optimizer=optim, loss=loss_func, metrics=[accc])\nhist = sentiment_model.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=epocas\n)\n\n# Guardando\nsentiment_model.save(nombre_modelo)\n\n# Cargando\nsentiment_model = tf.keras.models.load_model(nombre_modelo)\ntokenizer = BertTokenizer.from_pretrained(modelo)\n\n### Evaluando lo obtenido\n# Funciones\ndef prepare_data(input_text, tokenizer):\n    token = tokenizer.encode_plus(\n        input_text,\n        max_length=256, \n        truncation=True, \n        padding='max_length', \n        add_special_tokens=True,\n        return_tensors='tf'\n    )\n    return {\n        'input_ids': tf.cast(token.input_ids, tf.float64),\n        'attention_mask': tf.cast(token.attention_mask, tf.float64)\n    }\n\ndef make_prediction(model, processed_data, classes=etiquetas):\n    probs = model.predict(processed_data)[0]\n    return classes[np.argmax(probs)]\n\n# Evaluando general\npd.DataFrame(hist.history).plot(figsize=(8,5))\nplt.grid(True)\nplt.gca().set_ylim(0,1)\nplt.show()\n\n# Evaluando específico\ninput_text = 'That cristian girl is a bitch'\nprocessed_data = prepare_data(input_text, tokenizer)\nresult = make_prediction(sentiment_model, processed_data=processed_data)\nprint(f\"Predicted Sentiment: {result}\")\n#!zip -r /content/file.zip /content/bullying_beto_modelo\n\ny_pred = []\np=1\nfor i in X_test:\n  #print(p)\n  processed_data = prepare_data(i, tokenizer)\n  result = make_prediction(sentiment_model, processed_data=processed_data)\n  #print(f\"Predicted Sentiment: {result}\")\n  if result==\"No\":\n    y_pred.append(0)\n  elif result==\"Edad\":\n    y_pred.append(1)\n  elif result==\"Género\":\n    y_pred.append(2)\n  elif result==\"Etnicidad\":\n    y_pred.append(3)\n  elif result==\"Religión\":\n    y_pred.append(4)\n  elif result==\"Otro tipo\":\n    y_pred.append(5)\n  p+=1\n\n# Para Excel\ncooler_confusion_matrix(y_test,y_pred,\"BERT clean\",etiquetas)\nprint(classification_report(y_test,y_pred, target_names=etiquetas))","metadata":{"id":"J_uAaPEr8zRw","trusted":true},"execution_count":null,"outputs":[]}]}